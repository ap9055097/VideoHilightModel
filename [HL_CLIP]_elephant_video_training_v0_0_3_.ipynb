{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNuKdpYSqbhTq98tpOlXmee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ap9055097/VideoHilightModel/blob/main/%5BHL_CLIP%5D_elephant_video_training_v0_0_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "mdBhMGLzWk9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUfGqG3COX1H",
        "outputId": "4d3f4d49-1333-4450-b3fb-6777290c387c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset List\n"
      ],
      "metadata": {
        "id": "rQI20s76WxYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_video_files_walk(start_dir, video_exts=None):\n",
        "    if video_exts is None:\n",
        "        # Common video file extensions\n",
        "        video_exts = {'.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm'}\n",
        "    video_paths = []\n",
        "    annotations_data = {}\n",
        "    count_bad = 0\n",
        "    max_bad = 1000000000\n",
        "    for root, dirs, files in os.walk(start_dir):\n",
        "        if count_bad >= max_bad:\n",
        "              break\n",
        "        for filename in files:\n",
        "            if count_bad >= max_bad:\n",
        "              break\n",
        "            _, ext = os.path.splitext(filename)\n",
        "            if ext.lower() in video_exts:\n",
        "                path = os.path.join(root, filename)\n",
        "                video_id = os.path.splitext(os.path.basename(path))[0]\n",
        "                # print('video_id', video_id)\n",
        "                # print('path', path)\n",
        "                video_paths.append(path)\n",
        "                if \"/bad/\" in path:\n",
        "                  annotations_data[video_id] = []\n",
        "                  count_bad += 1\n",
        "    return video_paths, annotations_data\n",
        "\n",
        "\n",
        "directory_to_search = \"/content/drive/MyDrive/elephant_clip/train_augmented\"\n",
        "train_video_paths, train_annotations_data = list_video_files_walk(directory_to_search)\n",
        "# train_video_paths, train_annotations_data"
      ],
      "metadata": {
        "id": "XaOmmjsKbbLc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "goods = []\n",
        "bads = []\n",
        "for path in train_video_paths:\n",
        "    if \"/bad/\" in path:\n",
        "        bads.append(path)\n",
        "    else:\n",
        "        goods.append(path)\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(goods)\n",
        "random.shuffle(bads)\n",
        "\n",
        "len(goods), len(bads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxY6Yjp8swMd",
        "outputId": "998bb7cd-950b-4f26-8841-3cf8ddc3c423"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 240)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = 10\n",
        "train_video_paths = goods[:num_train_samples] + bads[:num_train_samples]\n",
        "len(train_video_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzYvrWbZtNgk",
        "outputId": "bb180263-e56e-4015-8d23-f3402be97bd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_to_search = \"/content/drive/MyDrive/elephant_clip/val\"\n",
        "val_video_paths, val_annotations_data = list_video_files_walk(directory_to_search)\n",
        "val_video_paths, val_annotations_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8BIsJ5ZpnYJ",
        "outputId": "a77a1f18-96f3-4306-badd-21987fedc882"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/drive/MyDrive/elephant_clip/val/good/Copy of 3QbFg0aff2Q.webm',\n",
              "  '/content/drive/MyDrive/elephant_clip/val/bad/GR7tNq9l4R8.webm',\n",
              "  '/content/drive/MyDrive/elephant_clip/val/bad/VcTaIc98_eE.webm'],\n",
              " {'GR7tNq9l4R8': [], 'VcTaIc98_eE': []})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install Dependencies"
      ],
      "metadata": {
        "id": "-mWMKeUkW34d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB_zgHtZPqXM",
        "outputId": "0e69c3cf-f75d-43de-996d-ecdbacb8736c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8ouih_4q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8ouih_4q\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=393f7bd17c52cf4d7df18bc4d178534cd0a7869c232bb28647ffd834ebdc2db9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m2j1u9_4/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zr_p6wvaLLk",
        "outputId": "765180da-d554-463c-f305-3cf02bc746b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-tensorrt in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: torch<2.8.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (2.7.0)\n",
            "Requirement already satisfied: tensorrt<10.10.0,>=10.9.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (10.9.0.34)\n",
            "Requirement already satisfied: tensorrt-cu12<10.10.0,>=10.9.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (10.9.0.34)\n",
            "Requirement already satisfied: tensorrt-cu12-bindings<10.10.0,>=10.9.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (10.9.0.34)\n",
            "Requirement already satisfied: tensorrt-cu12-libs<10.10.0,>=10.9.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (10.9.0.34)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from torch-tensorrt) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt-cu12-libs<10.10.0,>=10.9.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.8.0,>=2.7.0->torch-tensorrt) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch<2.8.0,>=2.7.0->torch-tensorrt) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<2.8.0,>=2.7.0->torch-tensorrt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.8.0,>=2.7.0->torch-tensorrt) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Model"
      ],
      "metadata": {
        "id": "gYIH8L1sXA0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. Import Libraries\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "# Potentially import from the HL-CLIP repository or your adapted versions\n",
        "# from hl_clip_model import HLCLIP # Assuming you create/adapt this"
      ],
      "metadata": {
        "id": "4nyTDrX_PBpj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "5e423aed-37fe-4781-e252-e7c5523e5d6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a6f97a58a1d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 0. Import Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2 # OpenCV for video processing\n",
        "from PIL import Image # For image manipulation\n",
        "import os # For path manipulation\n",
        "# numpy might be implicitly needed by cv2 or for other operations, good to have.\n",
        "# import numpy as np\n",
        "\n",
        "class ElephantHighlightDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading video clips and their highlight annotations.\n",
        "\n",
        "    If a video_id is not present in the annotations, the entire video is\n",
        "    considered a highlight.\n",
        "    If a video_id is present and its annotation is an empty list [],\n",
        "    it means the video is annotated but has no specific highlight segments.\n",
        "    \"\"\"\n",
        "    ENTIRE_VIDEO_IS_HIGHLIGHT_MARKER = \"ENTIRE_VIDEO_HIGHLIGHT\"  # Class attribute marker\n",
        "\n",
        "    def __init__(self, video_paths, annotations, transform=None, frames_per_clip=16, stride=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            video_paths (list): List of paths to video files.\n",
        "            annotations (dict or None): A dictionary mapping video_id (str, typically filename without extension)\n",
        "                                        to a list of [start_sec, end_sec] highlight segments.\n",
        "                                        If None, or if a video_id is missing, the entire video is a highlight.\n",
        "                                        An empty list [] for a video_id means that video has no highlights.\n",
        "            transform (callable, optional): A function/transform to apply to each frame (PIL Image).\n",
        "                                            Expected to return a tensor.\n",
        "            frames_per_clip (int): The number of frames to extract per clip.\n",
        "            stride (int): The step size (in frames) to sample frames from the video.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        # Handle cases where annotations might be None by treating it as an empty dict\n",
        "        self.annotations = annotations if annotations is not None else {}\n",
        "        self.transform = transform\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        self.stride = stride\n",
        "\n",
        "        if self.stride <= 0:\n",
        "            raise ValueError(\"Stride must be a positive integer.\")\n",
        "\n",
        "        self.clips = self._create_clips()\n",
        "\n",
        "    def _create_clips(self):\n",
        "        \"\"\"\n",
        "        Prepares a list of clips to be loaded. Each \"clip\" currently corresponds\n",
        "        to one video, from which `frames_per_clip` frames will be extracted.\n",
        "        Determines if the whole video is a highlight based on annotations.\n",
        "        \"\"\"\n",
        "        clips_data = []\n",
        "        for video_path in self.video_paths:\n",
        "            if not os.path.exists(video_path):\n",
        "                print(f\"Warning: Video path not found, skipping: {video_path}\")\n",
        "                continue\n",
        "\n",
        "            video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "            if video_id in self.annotations:\n",
        "                # Annotation exists for this video\n",
        "                highlight_segments_info = self.annotations[video_id]\n",
        "                # If it's an empty list [], it explicitly means no highlights for this (annotated) video.\n",
        "            else:\n",
        "                # No annotation provided for this video_id, so the entire video is a highlight.\n",
        "                highlight_segments_info = self.ENTIRE_VIDEO_IS_HIGHLIGHT_MARKER\n",
        "\n",
        "            clips_data.append({\n",
        "                \"video_path\": video_path,\n",
        "                \"video_id\": video_id,\n",
        "                \"highlight_segments_info\": highlight_segments_info\n",
        "            })\n",
        "        return clips_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a clip (sequence of frames) and its corresponding frame-level labels.\n",
        "        \"\"\"\n",
        "        clip_info = self.clips[idx]\n",
        "        video_path = clip_info[\"video_path\"]\n",
        "        highlight_segments_info = clip_info[\"highlight_segments_info\"]\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Warning: Could not open video {video_path}. Skipping.\")\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames_in_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        if fps <= 0 or total_frames_in_video == 0:\n",
        "            print(f\"Warning: FPS ({fps:.2f}) or total frames ({total_frames_in_video}) is invalid for video {video_path}. Skipping.\")\n",
        "            cap.release()\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "\n",
        "        frames = []\n",
        "        frame_indices = [] # Stores the original frame index in the video\n",
        "\n",
        "        current_frames_extracted = 0\n",
        "        # Iterate through video frames with the specified stride\n",
        "        for frame_num in range(0, total_frames_in_video, self.stride):\n",
        "            if current_frames_extracted >= self.frames_per_clip:\n",
        "                break # Reached the desired number of frames for the clip\n",
        "\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                # Could not read frame (e.g., end of video sooner than expected or corruption)\n",
        "                break\n",
        "\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "            processed_frame = pil_image # Default if no transform\n",
        "            if self.transform:\n",
        "                processed_frame = self.transform(pil_image)\n",
        "\n",
        "            frames.append(processed_frame)\n",
        "            frame_indices.append(frame_num)\n",
        "            current_frames_extracted += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if not frames:\n",
        "            # No frames were extracted (e.g., video was shorter than first stride, or frames_per_clip is 0)\n",
        "            # print(f\"Warning: No frames extracted for {video_path}.\") # Can be noisy if expected for very short videos\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "\n",
        "        # Generate frame-level labels\n",
        "        frame_labels = torch.zeros(len(frames), dtype=torch.float32)\n",
        "\n",
        "        if highlight_segments_info == self.ENTIRE_VIDEO_IS_HIGHLIGHT_MARKER:\n",
        "            # Entire video is a highlight, so all extracted frames are highlights\n",
        "            frame_labels = torch.ones(len(frames), dtype=torch.float32)\n",
        "        elif isinstance(highlight_segments_info, list):\n",
        "            # Specific highlight segments are provided (or an empty list for no highlights)\n",
        "            if not highlight_segments_info: # Empty list means no highlights\n",
        "                pass # frame_labels remain all zeros\n",
        "            else:\n",
        "                for i, sampled_frame_original_idx in enumerate(frame_indices):\n",
        "                    frame_time_sec = sampled_frame_original_idx / fps\n",
        "                    for start_sec, end_sec in highlight_segments_info:\n",
        "                        if start_sec <= frame_time_sec <= end_sec:\n",
        "                            frame_labels[i] = 1.0\n",
        "                            break # This frame is a highlight, move to the next frame\n",
        "        else:\n",
        "            # This case should ideally not be reached if _create_clips is correct\n",
        "            print(f\"Warning: Unexpected highlight_segments_info '{highlight_segments_info}' for {video_path}. Defaulting to no highlights.\")\n",
        "            # frame_labels remain all zeros\n",
        "\n",
        "        # Stack frames into a tensor\n",
        "        # This assumes that `self.transform` (if provided) converts PIL Images to PyTorch Tensors\n",
        "        # and ensures they are of the same size. If `self.transform` is None or doesn't\n",
        "        # produce tensors, torch.stack will fail.\n",
        "        try:\n",
        "            frames_tensor = torch.stack(frames)\n",
        "        except TypeError as e:\n",
        "            # Common if 'frames' contains PIL Images (i.e., self.transform was None or didn't convert to tensor)\n",
        "            # For models like CLIP, self.transform (the preprocess function) *does* return a tensor.\n",
        "            print(f\"TypeError during torch.stack for {video_path}: {e}. \"\n",
        "                  f\"Ensure self.transform converts images to tensors of uniform shape.\")\n",
        "            # Attempt a fallback basic conversion if they are all PIL Images\n",
        "            if all(isinstance(f, Image.Image) for f in frames):\n",
        "                try:\n",
        "                    import torchvision.transforms.functional as TF\n",
        "                    tensor_frames = [TF.to_tensor(f) for f in frames]\n",
        "                    frames_tensor = torch.stack(tensor_frames)\n",
        "                    print(f\"Info: Fallback - Converted PIL Images to Tensors for {video_path}. \"\n",
        "                          \"Please ensure transform handles this properly for consistency.\")\n",
        "                except Exception as conv_e:\n",
        "                    print(f\"Error: Fallback conversion to tensor failed for {video_path}: {conv_e}\")\n",
        "                    return torch.empty(0), torch.empty(0)\n",
        "            else: # Some other TypeError, or mixed types\n",
        "                return torch.empty(0), torch.empty(0)\n",
        "        except RuntimeError as e: # Typically size mismatch if tensors are not uniform\n",
        "            print(f\"RuntimeError during torch.stack for {video_path}: {e}. \"\n",
        "                  f\"Ensure self.transform produces tensors of uniform shape.\")\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "\n",
        "        return frames_tensor, frame_labels"
      ],
      "metadata": {
        "id": "eqCHjtM-PgJP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define HLCLIPModel class here (using the version that calls self.clip_visual_encoder(x_frames_flat))\n",
        "# (Make sure this is the version from the previous successful correction for the AttributeError)\n",
        "class HLCLIPModel(torch.nn.Module):\n",
        "    def __init__(self, clip_visual_encoder, num_transformer_layers_to_unfreeze=2, hidden_dim=512, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.clip_visual_encoder = clip_visual_encoder # This is original_clip_model.visual\n",
        "\n",
        "        # Freeze most of CLIP's visual encoder\n",
        "        for param in self.clip_visual_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last N layers of the visual transformer\n",
        "        if hasattr(self.clip_visual_encoder, 'transformer') and hasattr(self.clip_visual_encoder.transformer, 'resblocks'): # For ViT models\n",
        "            if self.clip_visual_encoder.transformer.resblocks is not None:\n",
        "                try:\n",
        "                    total_layers = len(self.clip_visual_encoder.transformer.resblocks)\n",
        "                    for i in range(max(0, total_layers - num_transformer_layers_to_unfreeze), total_layers):\n",
        "                        for param in self.clip_visual_encoder.transformer.resblocks[i].parameters():\n",
        "                            param.requires_grad = True\n",
        "                except TypeError:\n",
        "                    print(\"Warning: Could not unfreeze transformer resblocks. Check model structure.\")\n",
        "            else:\n",
        "                print(\"Warning: self.clip_visual_encoder.transformer.resblocks is None. Cannot unfreeze.\")\n",
        "        elif hasattr(self.clip_visual_encoder, 'layer4'): # Example for ResNet-based CLIP (e.g., layer4)\n",
        "            # Unfreeze layer4 and the attnpool\n",
        "            if hasattr(self.clip_visual_encoder, 'attnpool'):\n",
        "                 for param in self.clip_visual_encoder.attnpool.parameters():\n",
        "                        param.requires_grad = True\n",
        "            for param in self.clip_visual_encoder.layer4.parameters(): # Unfreeze layer4\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            print(\"Warning: Could not determine how to unfreeze layers for the provided clip_visual_encoder.\")\n",
        "\n",
        "        visual_output_dim = self.clip_visual_encoder.output_dim\n",
        "        self.fc = torch.nn.Linear(visual_output_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.output_layer = torch.nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x_frames): # x_frames: (batch_size, num_frames, C, H, W)\n",
        "        batch_size, num_frames, C, H, W = x_frames.shape\n",
        "\n",
        "        # Ensure x_frames is on the same device and dtype as the model expects for the visual encoder.\n",
        "        # The input x_frames should already be float32 from preprocess and moved to device in training loop.\n",
        "        x_frames_flat = x_frames.view(batch_size * num_frames, C, H, W)\n",
        "\n",
        "        # Debug: Check dtypes right before the problematic call\n",
        "        # if hasattr(self.clip_visual_encoder, 'conv1'): # For ViT\n",
        "        #     print(f\"DEBUG HLCLIPModel forward: x_frames_flat dtype: {x_frames_flat.dtype}, \"\n",
        "        #           f\"conv1 weight dtype: {self.clip_visual_encoder.conv1.weight.dtype}\")\n",
        "        # elif hasattr(self.clip_visual_encoder, 'stem') and len(self.clip_visual_encoder.stem) > 0: # For ResNet\n",
        "        #      print(f\"DEBUG HLCLIPModel forward: x_frames_flat dtype: {x_frames_flat.dtype}, \"\n",
        "        #       f\"stem[0] weight dtype: {self.clip_visual_encoder.stem[0].weight.dtype}\")\n",
        "\n",
        "\n",
        "        frame_features = self.clip_visual_encoder(x_frames_flat)\n",
        "\n",
        "        # Ensure features are float32 before passing to custom FC layers\n",
        "        frame_features = frame_features.to(torch.float32)\n",
        "\n",
        "        frame_features = frame_features.view(batch_size, num_frames, -1)\n",
        "        out = self.fc(frame_features)\n",
        "        out = self.relu(out)\n",
        "        highlight_scores = self.output_layer(out)\n",
        "        return torch.sigmoid(highlight_scores.squeeze(-1))"
      ],
      "metadata": {
        "id": "GoCT-9KUeiHk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Video Augmentation (conceptual)\n",
        "#    - Implemented within the Dataset's __getitem__ or via torchvision.transforms for frames"
      ],
      "metadata": {
        "id": "sVPsNi9XepA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "55cHhqbvXGcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define collate_fn here (ensure it's the latest version)\n",
        "# ... (your collate_fn code)\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None and b[0].nelement() > 0 and b[1].nelement() > 0]\n",
        "    if not batch:\n",
        "        return None, None\n",
        "    frames_list, labels_list = zip(*batch)\n",
        "    padded_frames = torch.nn.utils.rnn.pad_sequence(frames_list, batch_first=True, padding_value=0.0)\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels_list, batch_first=True, padding_value=0.0)\n",
        "    return padded_frames, padded_labels\n",
        "\n",
        "\n",
        "# # --- Main Setup ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model_name = \"ViT-B/32\"\n",
        "\n",
        "print(f\"Loading CLIP model {clip_model_name} to CPU...\")\n",
        "# 1. Load to CPU\n",
        "original_clip_model, preprocess = clip.load(clip_model_name, device=\"cpu\", jit=False)\n",
        "print(f\"CLIP model loaded to CPU. Initial dtype: {next(original_clip_model.parameters()).dtype}\")\n",
        "\n",
        "# 2. Convert to float32 while on CPU\n",
        "original_clip_model = original_clip_model.float()\n",
        "print(f\"CLIP model converted to float32 on CPU. New dtype: {next(original_clip_model.parameters()).dtype}\")\n",
        "\n",
        "# 3. Move the float32 model to the target device\n",
        "original_clip_model = original_clip_model.to(device)\n",
        "print(f\"CLIP model moved to {device}. Dtype on {device}: {next(original_clip_model.parameters()).dtype}\")\n",
        "\n",
        "# Extract the visual part (which should now be float32 and on the correct device)\n",
        "visual_encoder_component = original_clip_model.visual\n",
        "\n",
        "# Debug: Check the dtype of a specific layer in the visual_encoder_component\n",
        "if hasattr(visual_encoder_component, 'conv1'): # For ViT\n",
        "    print(f\"Visual encoder (ViT) conv1.weight dtype: {visual_encoder_component.conv1.weight.dtype}\")\n",
        "elif hasattr(visual_encoder_component, 'stem') and isinstance(visual_encoder_component.stem[0], torch.nn.Conv2d): # For ResNet\n",
        "    print(f\"Visual encoder (ResNet) stem[0].weight dtype: {visual_encoder_component.stem[0].weight.dtype}\")\n",
        "\n",
        "\n",
        "# 4. Instantiate your HLCLIPModel\n",
        "model = HLCLIPModel(visual_encoder_component, num_transformer_layers_to_unfreeze=2).to(device)\n",
        "# The .to(device) on HLCLIPModel should not change dtypes if its components are already correct.\n",
        "\n",
        "print(f\"HLCLIPModel instantiated on {device}.\")\n",
        "# Debug: Check dtypes within the instantiated HLCLIPModel\n",
        "if hasattr(model.clip_visual_encoder, 'conv1'): # For ViT\n",
        "    print(f\"  HLCLIPModel's visual_encoder.conv1.weight dtype: {model.clip_visual_encoder.conv1.weight.dtype}\")\n",
        "elif hasattr(model.clip_visual_encoder, 'stem') and isinstance(model.clip_visual_encoder.stem[0], torch.nn.Conv2d): # For ResNet\n",
        "     print(f\"  HLCLIPModel's visual_encoder.stem[0].weight dtype: {model.clip_visual_encoder.stem[0].weight.dtype}\")\n",
        "print(f\"  HLCLIPModel's fc.weight dtype: {model.fc.weight.dtype}\") # Custom FC layers should be float32 by default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJASl2AQgv_c",
        "outputId": "86c137e5-7124-468f-d15a-c0cd7a026a76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CLIP model ViT-B/32 to CPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 45.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP model loaded to CPU. Initial dtype: torch.float32\n",
            "CLIP model converted to float32 on CPU. New dtype: torch.float32\n",
            "CLIP model moved to cuda. Dtype on cuda: torch.float32\n",
            "Visual encoder (ViT) conv1.weight dtype: torch.float32\n",
            "HLCLIPModel instantiated on cuda.\n",
            "  HLCLIPModel's visual_encoder.conv1.weight dtype: torch.float32\n",
            "  HLCLIPModel's fc.weight dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dataset"
      ],
      "metadata": {
        "id": "vrCeTbjYXM-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Dummy data and DataLoader (ensure ElephantHighlightDataset and video creation is correct)\n",
        "# ... (your dummy data setup as in the previous response, ensure videos are valid and long enough)\n",
        "# dummy_video_paths = [\"dummy_video1.mp4\", \"dummy_video2.mp4\"] # Create small dummy mp4 files for testing\n",
        "# dummy_annotations = {\"dummy_video1\": [[5, 10]], \"dummy_video2\": [[2, 6], [15, 20]]}\n",
        "\n",
        "# train_video_paths\n",
        "# train_annotations_data\n",
        "\n",
        "for p in train_video_paths:\n",
        "    if not os.path.exists(p):\n",
        "        try:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out_dummy = cv2.VideoWriter(p, fourcc, 30, (224, 224))\n",
        "            for _ in range(30 * 5): # 5 seconds of video\n",
        "                frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "                out_dummy.write(frame)\n",
        "            out_dummy.release()\n",
        "        except Exception as e: print(f\"Error creating dummy video {p}: {e}\")\n",
        "\n",
        "train_dataset = ElephantHighlightDataset(train_video_paths, train_annotations_data, transform=preprocess, frames_per_clip=32, stride=15)\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty! Check video paths, annotations, or dataset creation logic.\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "EXi4iqgCicFI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for p in val_video_paths:\n",
        "    if not os.path.exists(p):\n",
        "        try:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out_dummy = cv2.VideoWriter(p, fourcc, 30, (224, 224))\n",
        "            for _ in range(30 * 5): # 5 seconds of video\n",
        "                frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "                out_dummy.write(frame)\n",
        "            out_dummy.release()\n",
        "        except Exception as e: print(f\"Error creating dummy video {p}: {e}\")\n",
        "\n",
        "validation_dataset = ElephantHighlightDataset(val_video_paths, val_annotations_data, transform=preprocess, frames_per_clip=32, stride=15)\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty! Check video paths, annotations, or dataset creation logic.\")\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "if len(validation_dataset) == 0:\n",
        "    print(\"Warning: Validation dataset is empty! Check paths or dataset logic.\")\n",
        "    # Handle this case, perhaps by skipping validation or raising an error earlier\n",
        "    validation_loader = None\n",
        "else:\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset,\n",
        "        batch_size=2, # Adjust batch size as needed\n",
        "        shuffle=False, # No need to shuffle for validation\n",
        "        collate_fn=collate_fn # Use the same collate_fn\n",
        "    )"
      ],
      "metadata": {
        "id": "DruLp61yqEMr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF5fbCE7pTTB",
        "outputId": "9a787683-ff8f-49fd-f927-d736a4619d1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "CYv0c8kwXSIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "def calculate_frame_metrics(all_preds_flat, all_labels_flat, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates frame-level precision, recall, and F1-score.\n",
        "    Args:\n",
        "        all_preds_flat (np.array): 1D array of predicted probabilities.\n",
        "        all_labels_flat (np.array): 1D array of ground truth labels (0 or 1).\n",
        "        threshold (float): Threshold to convert probabilities to binary predictions.\n",
        "    Returns:\n",
        "        dict: Dictionary containing precision, recall, f1, and confusion matrix.\n",
        "    \"\"\"\n",
        "    if len(all_preds_flat) == 0 or len(all_labels_flat) == 0:\n",
        "        return {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0, \"accuracy\": 0}\n",
        "\n",
        "    binary_preds = (all_preds_flat >= threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(all_labels_flat, binary_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels_flat, binary_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels_flat, binary_preds, zero_division=0)\n",
        "\n",
        "    # Confusion matrix: tn, fp, fn, tp\n",
        "    # Ensure labels=[0, 1] to handle cases where one class might be missing in a small batch/dataset\n",
        "    # For a large enough dataset, this is less of an issue.\n",
        "    # If binary_preds or all_labels_flat only contain one class, confusion_matrix might return a 1x1 matrix.\n",
        "    unique_labels = np.unique(np.concatenate((all_labels_flat, binary_preds)))\n",
        "    if len(unique_labels) == 1: # Only one class present in both true and pred\n",
        "        if unique_labels[0] == 0: # Only negatives\n",
        "            tn = len(all_labels_flat)\n",
        "            fp, fn, tp = 0,0,0\n",
        "        else: # Only positives\n",
        "            tp = len(all_labels_flat)\n",
        "            tn, fp, fn = 0,0,0\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "    else: # Both classes are potentially present\n",
        "        cm = confusion_matrix(all_labels_flat, binary_preds, labels=[0, 1])\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "\n",
        "\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp, \"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "b3zxly-iqVL0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, criterion, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Validates the model on the validation set.\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to validate.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        device (torch.device): The device to run validation on.\n",
        "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
        "    Returns:\n",
        "        tuple: (average_validation_loss, metrics_dict)\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    all_predictions_list = []\n",
        "    all_labels_list = []\n",
        "    batches_processed = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for batch_idx, batch_data in enumerate(tqdm(val_loader, desc=\"Validating\")):\n",
        "            if batch_data is None or batch_data[0] is None:\n",
        "                print(f\"Skipping empty or invalid validation batch {batch_idx} from collate_fn.\")\n",
        "                continue\n",
        "\n",
        "            video_frames, labels = batch_data\n",
        "\n",
        "            if video_frames.nelement() == 0:\n",
        "                print(f\"Skipping validation batch {batch_idx} due to empty video_frames tensor.\")\n",
        "                continue\n",
        "\n",
        "            video_frames = video_frames.to(device)\n",
        "            labels = labels.to(device) # Shape: (batch_size, num_frames)\n",
        "\n",
        "            predictions = model(video_frames) # Shape: (batch_size, num_frames)\n",
        "\n",
        "            min_len = min(predictions.shape[1], labels.shape[1])\n",
        "            if min_len == 0:\n",
        "                 print(f\"Skipping validation batch {batch_idx} due to min_len=0.\")\n",
        "                 continue\n",
        "            predictions_aligned = predictions[:, :min_len]\n",
        "            labels_aligned = labels[:, :min_len]\n",
        "\n",
        "            if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0:\n",
        "                print(f\"Skipping validation batch {batch_idx} due to zero elements after alignment.\")\n",
        "                continue\n",
        "\n",
        "            loss = criterion(predictions_aligned, labels_aligned)\n",
        "            total_val_loss += loss.item()\n",
        "            batches_processed += 1\n",
        "\n",
        "            # Store predictions and labels for overall metric calculation\n",
        "            all_predictions_list.append(predictions_aligned.cpu().numpy().flatten())\n",
        "            all_labels_list.append(labels_aligned.cpu().numpy().flatten())\n",
        "\n",
        "    if batches_processed == 0:\n",
        "        print(\"No batches were processed during validation. Returning zero metrics.\")\n",
        "        return 0, {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0, \"accuracy\": 0}\n",
        "\n",
        "    avg_val_loss = total_val_loss / batches_processed\n",
        "\n",
        "    # Concatenate all predictions and labels\n",
        "    all_preds_flat = np.concatenate(all_predictions_list)\n",
        "    all_labels_flat = np.concatenate(all_labels_list)\n",
        "\n",
        "    # print('all_preds_flat', all_preds_flat)\n",
        "    # print('all_labels_flat', all_labels_flat)\n",
        "\n",
        "    metrics = calculate_frame_metrics(all_preds_flat, all_labels_flat, threshold)\n",
        "\n",
        "    return avg_val_loss, metrics"
      ],
      "metadata": {
        "id": "mwhv05kAqaip"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "KlEyFPJrXXJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Your existing training setup: model, optimizer, criterion, train_loader, etc.)\n",
        "# ... (HLCLIPModel, ElephantHighlightDataset, collate_fn, preprocess, device etc. need to be defined)\n",
        "# ... (Load CLIP, convert to float32, instantiate HLCLIPModel, optimizer, criterion as before)\n",
        "# ... (Create dummy train/val videos, train_dataset, train_loader, validation_dataset, validation_loader as shown above)\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "print(\"Starting training and validation loop...\")\n",
        "num_epochs = 4 # Example\n",
        "best_val_f1 = -1 # To save the best model based on F1-score, for example\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "    train_batches_processed = 0\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")):\n",
        "        # ... (your existing training batch logic) ...\n",
        "        if batch_data is None or batch_data[0] is None:\n",
        "            print(f\"Skipping empty or invalid training batch {batch_idx} from collate_fn.\")\n",
        "            continue\n",
        "        video_frames, labels = batch_data\n",
        "        if video_frames.nelement() == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to empty video_frames tensor.\")\n",
        "            continue\n",
        "\n",
        "        video_frames = video_frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(video_frames)\n",
        "\n",
        "        min_len = min(predictions.shape[1], labels.shape[1])\n",
        "        if min_len == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to min_len=0.\")\n",
        "            continue\n",
        "        predictions_aligned = predictions[:, :min_len]\n",
        "        labels_aligned = labels[:, :min_len]\n",
        "\n",
        "        if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to zero elements post-alignment.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            loss = criterion(predictions_aligned, labels_aligned)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "            train_batches_processed +=1\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Runtime error during training loss/backward for batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else 0\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Validation Step ---\n",
        "    if validation_loader: # Only run validation if the loader was successfully created\n",
        "        avg_val_loss, val_metrics = validate_model(model, validation_loader, criterion, device, threshold=0.5)\n",
        "        print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Epoch {epoch+1} - Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1-Score: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {val_metrics['tn']}, FP: {val_metrics['fp']}, FN: {val_metrics['fn']}, TP: {val_metrics['tp']}\")\n",
        "\n",
        "        # Example: Save the model if validation F1 improves\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            torch.save(model.state_dict(), \"best_hlclip_model.pth\")\n",
        "            print(f\"New best validation F1: {best_val_f1:.4f}. Model saved to best_hlclip_model.pth\")\n",
        "    else:\n",
        "        print(\"Skipping validation as validation_loader is not available.\")\n",
        "\n",
        "print(\"Training and validation finished.\")\n",
        "\n",
        "# To load the best model later:\n",
        "# model.load_state_dict(torch.load(\"best_hlclip_model.pth\"))\n",
        "# model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dafPFMZjB0a",
        "outputId": "8f32886c-59af-4ebd-e006-b0c82a6625c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training and validation loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4 [Training]: 100%|██████████| 10/10 [04:02<00:00, 24.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training Loss: 0.6763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:48<00:00, 24.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_preds_flat [0.5225878  0.54144233 0.5433463  0.5410959  0.53662497 0.49992406\n",
            " 0.559193   0.5454702  0.5353346  0.54724556 0.5501838  0.5347098\n",
            " 0.5460341  0.5324557  0.5362321  0.5041454  0.5348347  0.519287\n",
            " 0.51749    0.5178836  0.51788694 0.51351464 0.5105569  0.5072191\n",
            " 0.5178161  0.5045232  0.5186914  0.51890737 0.5224287  0.5324302\n",
            " 0.48861927 0.48861927 0.49503893 0.5002784  0.49716362 0.50254893\n",
            " 0.4864476  0.48269314 0.51476324 0.5098843  0.5060159  0.5097177\n",
            " 0.49808812 0.4823548  0.50461835 0.5107068  0.50905305 0.5004153\n",
            " 0.51755697 0.50610894 0.49011618 0.5092763  0.50397706 0.5044606\n",
            " 0.51187783 0.49146077 0.5078479  0.49737632 0.49865732 0.48125315\n",
            " 0.50287664 0.4976621  0.51336175 0.50813884 0.5458212  0.52729243\n",
            " 0.53914726 0.5425164  0.5213971  0.53712195 0.53189474 0.5348592\n",
            " 0.5402713  0.537704   0.53379744 0.5518986  0.5391309  0.543879\n",
            " 0.55345356 0.55518687 0.540268   0.553546   0.5417495  0.5445411\n",
            " 0.5455856  0.54110503 0.5509486  0.5522598  0.5466699  0.546418\n",
            " 0.5381131  0.5515571  0.5483257  0.5500223  0.5546534  0.5546219 ]\n",
            "all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1 - Validation Loss: 0.7269\n",
            "Epoch 1 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.3580, Recall: 0.9667, F1-Score: 0.5225, Accuracy: 0.4479\n",
            "  TN: 14, FP: 52, FN: 1, TP: 29\n",
            "New best validation F1: 0.5225. Model saved to best_hlclip_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4 [Training]: 100%|██████████| 10/10 [03:57<00:00, 23.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Training Loss: 0.5879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:50<00:00, 25.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_preds_flat [0.49229982 0.5300647  0.53448796 0.5269284  0.52833533 0.46934995\n",
            " 0.5693245  0.54603404 0.5296376  0.55300194 0.55835193 0.5373898\n",
            " 0.5549976  0.5307724  0.53981495 0.48922786 0.5395259  0.5156486\n",
            " 0.5092648  0.509992   0.5047577  0.49144706 0.48359683 0.48291287\n",
            " 0.5010469  0.48271745 0.499987   0.49753904 0.5142505  0.5347646\n",
            " 0.46783647 0.46783647 0.4630325  0.45889953 0.4586705  0.4672962\n",
            " 0.45262936 0.4451465  0.48465666 0.48247823 0.46731573 0.47493693\n",
            " 0.45775962 0.44172227 0.47308773 0.48335359 0.48338193 0.47192562\n",
            " 0.48782918 0.47792688 0.4460721  0.46874395 0.46568018 0.4648175\n",
            " 0.47230724 0.45310527 0.46675238 0.46869168 0.45746106 0.43710217\n",
            " 0.4671564  0.45720315 0.47472784 0.46913737 0.55543625 0.52627134\n",
            " 0.54053456 0.55532515 0.52102447 0.541468   0.53355527 0.5395914\n",
            " 0.5489915  0.5447748  0.5395002  0.56386584 0.54514015 0.5539426\n",
            " 0.5692381  0.56909066 0.547701   0.56594175 0.5565082  0.5577815\n",
            " 0.5580247  0.5584157  0.5684799  0.57370746 0.5603002  0.56023586\n",
            " 0.55268854 0.5725119  0.56676406 0.56923336 0.5737707  0.5757256 ]\n",
            "all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 2 - Validation Loss: 0.7261\n",
            "Epoch 2 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.3962, Recall: 0.7000, F1-Score: 0.5060, Accuracy: 0.5729\n",
            "  TN: 34, FP: 32, FN: 9, TP: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4 [Training]: 100%|██████████| 10/10 [04:01<00:00, 24.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Training Loss: 0.5030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:50<00:00, 25.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_preds_flat [0.48368272 0.5574484  0.5646968  0.5519394  0.5658476  0.4642303\n",
            " 0.62228924 0.5918364  0.56605476 0.6088873  0.6131705  0.5815168\n",
            " 0.60929495 0.5755058  0.59179986 0.50738364 0.5892433  0.5506633\n",
            " 0.54183006 0.5418832  0.52110225 0.4985219  0.47877932 0.48107255\n",
            " 0.51604664 0.48508927 0.5153759  0.5107049  0.5439264  0.58580285\n",
            " 0.45771676 0.45771676 0.4580974  0.4454532  0.45549974 0.45926523\n",
            " 0.43539637 0.42356423 0.4897215  0.49396044 0.45619145 0.47368452\n",
            " 0.4440367  0.4230054  0.47139046 0.48580423 0.48680198 0.48150086\n",
            " 0.49327692 0.4877202  0.42804542 0.46086726 0.45439816 0.45859176\n",
            " 0.46789047 0.4427486  0.45802346 0.47351313 0.44701073 0.41527146\n",
            " 0.45452726 0.4436142  0.46248385 0.45804015 0.6198452  0.5757568\n",
            " 0.59267724 0.62432015 0.572741   0.59974176 0.5887372  0.59470767\n",
            " 0.6095875  0.6072572  0.6011942  0.62911725 0.6057438  0.6180363\n",
            " 0.6355566  0.6311656  0.6035733  0.6283551  0.62434983 0.6210126\n",
            " 0.61938596 0.6230009  0.63155407 0.6398124  0.6254445  0.6248393\n",
            " 0.6187688  0.6413106  0.63601613 0.6374079  0.6431224  0.6450972 ]\n",
            "all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 3 - Validation Loss: 0.7869\n",
            "Epoch 3 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.4286, Recall: 0.8000, F1-Score: 0.5581, Accuracy: 0.6042\n",
            "  TN: 34, FP: 32, FN: 6, TP: 24\n",
            "New best validation F1: 0.5581. Model saved to best_hlclip_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4 [Training]: 100%|██████████| 10/10 [04:00<00:00, 24.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Training Loss: 0.4217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:50<00:00, 25.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_preds_flat [0.44192407 0.5336282  0.5463551  0.52427584 0.54107386 0.40625402\n",
            " 0.6433243  0.59398293 0.5491024  0.6215835  0.6329871  0.5856069\n",
            " 0.62324435 0.56889945 0.59591806 0.47876215 0.5999918  0.54079115\n",
            " 0.5255064  0.5189118  0.4885289  0.458691   0.43948573 0.44201124\n",
            " 0.48507357 0.44857246 0.48176965 0.47468206 0.520445   0.5749314\n",
            " 0.4398734  0.4398734  0.41159803 0.38812122 0.38435784 0.3986417\n",
            " 0.39369828 0.37729093 0.43368763 0.44343388 0.39630863 0.409026\n",
            " 0.37910715 0.36814222 0.40856427 0.4363653  0.4361596  0.42703235\n",
            " 0.443478   0.4359329  0.36880374 0.39190745 0.39308414 0.3895821\n",
            " 0.3931911  0.380482   0.38573343 0.4123327  0.37736905 0.3611032\n",
            " 0.4031703  0.38353035 0.40637854 0.39925623 0.64753777 0.56401986\n",
            " 0.5786837  0.6502418  0.57666546 0.6112771  0.59065074 0.6054918\n",
            " 0.63216174 0.620038   0.60712534 0.6488185  0.61732006 0.6370628\n",
            " 0.67048395 0.6629286  0.6250067  0.65696317 0.65815955 0.65083086\n",
            " 0.6496995  0.65892935 0.66407377 0.6819296  0.6586268  0.6580058\n",
            " 0.65250415 0.6852533  0.6753889  0.6759369  0.68127996 0.6890619 ]\n",
            "all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 4 - Validation Loss: 0.8035\n",
            "Epoch 4 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.3725, Recall: 0.6333, F1-Score: 0.4691, Accuracy: 0.5521\n",
            "  TN: 34, FP: 32, FN: 11, TP: 19\n",
            "Training and validation finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxqvwXomoZL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "6nNxgwdgXc3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Production (Inference)\n",
        "def predict_highlights(video_path, model, transform, device, frames_per_clip=100, stride=5, threshold=0.5):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    extracted_frames_pil = []\n",
        "    frame_indices = []\n",
        "\n",
        "    for i in range(0, total_frames_video, stride):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(frame_rgb)\n",
        "        extracted_frames_pil.append(pil_image)\n",
        "        frame_indices.append(i)\n",
        "    cap.release()\n",
        "\n",
        "    if not extracted_frames_pil:\n",
        "        return []\n",
        "\n",
        "    all_highlight_scores = []\n",
        "    with torch.no_grad():\n",
        "        # Process in chunks if video is too long for memory\n",
        "        for i in range(0, len(extracted_frames_pil), frames_per_clip):\n",
        "            batch_pil_images = extracted_frames_pil[i:i+frames_per_clip]\n",
        "            batch_frames_tensor = torch.stack([transform(pil_img) for pil_img in batch_pil_images]).unsqueeze(0).to(device) # Add batch dim\n",
        "\n",
        "            scores = model(batch_frames_tensor) # (1, num_frames_in_batch)\n",
        "            all_highlight_scores.extend(scores.squeeze(0).cpu().numpy())\n",
        "\n",
        "    detected_highlights_segments = []\n",
        "    is_in_highlight = False\n",
        "    start_frame = 0\n",
        "    for i, score in enumerate(all_highlight_scores):\n",
        "        current_frame_index_in_video = frame_indices[i]\n",
        "        if score >= threshold and not is_in_highlight:\n",
        "            is_in_highlight = True\n",
        "            start_frame = current_frame_index_in_video\n",
        "        elif score < threshold and is_in_highlight:\n",
        "            is_in_highlight = False\n",
        "            end_frame = frame_indices[i-1] # Previous frame was the end\n",
        "            detected_highlights_segments.append((start_frame / fps, end_frame / fps))\n",
        "    if is_in_highlight: # If video ends during a highlight\n",
        "         detected_highlights_segments.append((start_frame / fps, frame_indices[-1] / fps))\n",
        "\n",
        "    return detected_highlights_segments\n",
        "\n",
        "# Example Inference:\n",
        "# test_video_path = \"path/to/your/elephant_video.mp4\"\n",
        "# highlights = predict_highlights(dummy_video_paths[0], model, preprocess, device)\n",
        "# print(f\"Detected highlights in {dummy_video_paths[0]}: {highlights}\")"
      ],
      "metadata": {
        "id": "CKs-PCEUjSLj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HLCLIPModel(visual_encoder_component, num_transformer_layers_to_unfreeze=2).to(device)\n",
        "model.load_state_dict(torch.load(\"best_hlclip_model.pth\", weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss7nfcaXPRgi",
        "outputId": "459bede1-f696-48ce-b85b-cdf1901143df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HLCLIPModel(\n",
              "  (clip_visual_encoder): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlights = predict_highlights(val_video_paths[0], model, preprocess, device)\n",
        "print(f\"Detected highlights in {val_video_paths[0]}: {highlights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCZc3cY3obBU",
        "outputId": "54c96bd1-e4b7-4909-b59e-fe6c2c74256a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected highlights in /content/drive/MyDrive/elephant_clip/val/good/Copy of 3QbFg0aff2Q.webm: [(0.33386752136752135, 2.170138888888889), (2.6709401709401708, 10.01602564102564), (11.852297008547009, 12.019230769230768), (13.020833333333332, 14.523237179487179)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlights = predict_highlights(val_video_paths[1], model, preprocess, device)\n",
        "print(f\"Detected highlights in {val_video_paths[1]}: {highlights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwbJZI4YLdF1",
        "outputId": "09ac7bc7-347c-4484-e449-93a7f4b34b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected highlights in /content/drive/MyDrive/elephant_clip/val/bad/GR7tNq9l4R8.webm: [(6.666398293144398, 6.666398293144398), (8.166337909101888, 8.332997866430498)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlights = predict_highlights(val_video_paths[2], model, preprocess, device)\n",
        "print(f\"Detected highlights in {val_video_paths[2]}: {highlights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWUHlxRTwL-B",
        "outputId": "0fac0c3a-9e56-44c1-cac4-2aa8c14b8422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected highlights in /content/drive/MyDrive/elephant_clip/val/bad/VcTaIc98_eE.webm: [(0.0, 60.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_video_paths[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4DZ3e_CEwCAD",
        "outputId": "a3f4b088-84f8-4f6c-f602-370b163353be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/splitted_elephant_videos/val/bad/formatted_b2.mp4clip_001.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4fIeCZewJgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBcFrGYUz-JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero shot model"
      ],
      "metadata": {
        "id": "vklgLQNFXjAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text prompts for elephant highlights\n",
        "# You should tailor these to what you consider highlights\n",
        "ZERO_SHOT_PROMPTS = [\n",
        "    \"a highlight in an elephant video\",\n",
        "    \"an interesting elephant behavior\",\n",
        "    \"a significant moment with elephants\",\n",
        "    \"an elephant showing unusual activity\",\n",
        "    \"a notable interaction between elephants\",\n",
        "    \"an important event involving an elephant\"\n",
        "]\n",
        "# You can also try simpler ones or even just \"a highlight\"\n",
        "# Experiment with what works best."
      ],
      "metadata": {
        "id": "sQHJ24YGz-MN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip # Ensure clip is imported\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix # If not already imported\n",
        "\n",
        "# Assume calculate_frame_metrics is defined as in the previous response\n",
        "# def calculate_frame_metrics(all_preds_flat, all_labels_flat, threshold=0.5): ...\n",
        "\n",
        "@torch.no_grad() # Decorator for no_grad context\n",
        "def validate_zero_shot_clip(original_clip_model, val_loader, text_prompts, device, threshold=0.5):\n",
        "    original_clip_model.eval()\n",
        "    all_predictions_list = []\n",
        "    all_labels_list = []\n",
        "    total_val_loss_dummy = 0 # Loss is not directly applicable here in the same way as supervised\n",
        "    batches_processed = 0\n",
        "\n",
        "    # Pre-encode text prompts for efficiency\n",
        "    tokenized_prompts = clip.tokenize(text_prompts).to(device)\n",
        "    text_features = original_clip_model.encode_text(tokenized_prompts)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True) # Normalize text features\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(val_loader, desc=\"Validating Zero-Shot CLIP\")):\n",
        "        if batch_data is None or batch_data[0] is None:\n",
        "            print(f\"Skipping empty ZS validation batch {batch_idx}.\")\n",
        "            continue\n",
        "        video_frames, labels = batch_data # video_frames shape: (B, N_frames, C, H, W)\n",
        "        if video_frames.nelement() == 0:\n",
        "            print(f\"Skipping ZS validation batch {batch_idx} due to empty video_frames.\")\n",
        "            continue\n",
        "\n",
        "        video_frames = video_frames.to(device)\n",
        "        labels = labels.to(device) # Shape: (B, N_frames)\n",
        "\n",
        "        batch_size, num_clip_frames, C, H, W = video_frames.shape\n",
        "        video_frames_flat = video_frames.view(batch_size * num_clip_frames, C, H, W)\n",
        "\n",
        "        # Get image features (these are already preprocessed by the dataset's transform)\n",
        "        image_features = original_clip_model.encode_image(video_frames_flat)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True) # Normalize image features\n",
        "\n",
        "        # Calculate similarity scores\n",
        "        # logit_scale is a learned parameter in CLIP\n",
        "        logit_scale = original_clip_model.logit_scale.exp()\n",
        "        similarity = (logit_scale * image_features @ text_features.T) # Shape: (B*N_frames, num_prompts)\n",
        "\n",
        "        # Convert similarity to \"probabilities\" or scores per frame.\n",
        "        # A common approach: take the max similarity across all positive prompts for each frame.\n",
        "        # If you have \"negative\" prompts, you might need a different strategy.\n",
        "        frame_highlight_scores = similarity.max(dim=1)[0] # Max similarity for each frame\n",
        "        # frame_highlight_scores = similarity.softmax(dim=-1)[:, 0] # if you prefer softmax & pick a specific prompt's prob\n",
        "\n",
        "        # Reshape scores back to (batch_size, num_clip_frames)\n",
        "        frame_highlight_scores = frame_highlight_scores.view(batch_size, num_clip_frames)\n",
        "\n",
        "        min_len = min(frame_highlight_scores.shape[1], labels.shape[1])\n",
        "        if min_len == 0:\n",
        "            print(f\"Skipping ZS batch {batch_idx} due to min_len=0.\")\n",
        "            continue\n",
        "\n",
        "        predictions_aligned = frame_highlight_scores[:, :min_len]\n",
        "        labels_aligned = labels[:, :min_len]\n",
        "\n",
        "        if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0:\n",
        "            print(f\"Skipping ZS batch {batch_idx} due to zero elements post-alignment.\")\n",
        "            continue\n",
        "\n",
        "        all_predictions_list.append(predictions_aligned.cpu().numpy().flatten())\n",
        "        all_labels_list.append(labels_aligned.cpu().numpy().flatten())\n",
        "        batches_processed += 1\n",
        "\n",
        "    if batches_processed == 0:\n",
        "        print(\"No batches processed during Zero-Shot validation.\")\n",
        "        return 0, {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0, \"accuracy\":0}\n",
        "\n",
        "    all_preds_flat = np.concatenate(all_predictions_list)\n",
        "    all_labels_flat = np.concatenate(all_labels_list)\n",
        "\n",
        "    # Note: The scores from CLIP similarity might not be in [0,1] like sigmoid outputs.\n",
        "    # `calculate_frame_metrics` applies a threshold. You might need to adjust the threshold\n",
        "    # or normalize CLIP scores to a [0,1] range if they behave very differently.\n",
        "    # For now, we use the same thresholding logic.\n",
        "    # print('zs all_preds_flat', all_preds_flat)\n",
        "    # print('zs all_labels_flat', all_labels_flat)\n",
        "    metrics = calculate_frame_metrics(all_preds_flat, all_labels_flat, threshold)\n",
        "\n",
        "    # A dummy loss (or you can skip loss for zero-shot as it's not directly comparable)\n",
        "    avg_val_loss_dummy = 0\n",
        "\n",
        "    return avg_val_loss_dummy, metrics"
      ],
      "metadata": {
        "id": "3EfV61R1z-de"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Your existing setup: ElephantHighlightDataset, HLCLIPModel, collate_fn, calculate_frame_metrics, validate_model)\n",
        "# ... (Make sure all necessary classes and functions are defined)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model_name = \"ViT-B/32\" # Or your chosen CLIP model\n",
        "\n",
        "# --- 1. Load Original CLIP Model for Zero-Shot ---\n",
        "print(\"Loading Original CLIP model for Zero-Shot evaluation...\")\n",
        "# Load to CPU first, convert to float, then to device (as per previous fixes for dtype)\n",
        "original_clip_model_zs, preprocess_zs = clip.load(clip_model_name, device=\"cpu\", jit=False)\n",
        "original_clip_model_zs = original_clip_model_zs.float()\n",
        "original_clip_model_zs = original_clip_model_zs.to(device)\n",
        "print(f\"Original CLIP model ({clip_model_name}) loaded to {device} with dtype {next(original_clip_model_zs.parameters()).dtype}\")\n",
        "\n",
        "# --- 2. Load Fine-Tuned HL-CLIP Model ---\n",
        "print(\"\\nLoading Fine-Tuned HL-CLIP model...\")\n",
        "# First, load the visual backbone the same way for consistency if HLCLIPModel expects it pre-converted\n",
        "original_clip_model_ft_base, preprocess_ft = clip.load(clip_model_name, device=\"cpu\", jit=False)\n",
        "original_clip_model_ft_base = original_clip_model_ft_base.float()\n",
        "original_clip_model_ft_base = original_clip_model_ft_base.to(device)\n",
        "visual_encoder_component_ft = original_clip_model_ft_base.visual\n",
        "\n",
        "fine_tuned_model = HLCLIPModel(visual_encoder_component_ft, num_transformer_layers_to_unfreeze=2).to(device) # Adjust params as needed\n",
        "\n",
        "# Define the path to your saved fine-tuned model weights\n",
        "fine_tuned_model_path = \"best_hlclip_model.pth\" # MAKE SURE THIS PATH IS CORRECT\n",
        "if os.path.exists(fine_tuned_model_path):\n",
        "    try:\n",
        "        fine_tuned_model.load_state_dict(torch.load(fine_tuned_model_path, map_location=device))\n",
        "        print(f\"Fine-tuned HL-CLIP model weights loaded from {fine_tuned_model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading fine-tuned model weights: {e}. Proceeding with uninitialized/pre-trained HL-CLIP head.\")\n",
        "else:\n",
        "    print(f\"Warning: Fine-tuned model weights not found at {fine_tuned_model_path}. Validating with uninitialized/pre-trained HL-CLIP head.\")\n",
        "fine_tuned_model.eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj5w0Yl8093T",
        "outputId": "82487071-8eb4-4d22-e898-e0218bd3d8bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original CLIP model for Zero-Shot evaluation...\n",
            "Original CLIP model (ViT-B/32) loaded to cuda with dtype torch.float32\n",
            "\n",
            "Loading Fine-Tuned HL-CLIP model...\n",
            "Fine-tuned HL-CLIP model weights loaded from best_hlclip_model.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HLCLIPModel(\n",
              "  (clip_visual_encoder): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Important: Use the preprocess from the model you are evaluating IF they differ.\n",
        "# For zero-shot, use preprocess_zs. For fine-tuned, if it used a different preprocess during its training,\n",
        "# that would be ideal. However, typically CLIP's preprocess is standard.\n",
        "# Here, we assume preprocess_zs and preprocess_ft are the same. If not, you might need two DataLoaders.\n",
        "validation_dataset = ElephantHighlightDataset(\n",
        "    val_video_paths, val_annotations_data, transform=preprocess_zs, # Using one preprocess\n",
        "    frames_per_clip=32, stride=15\n",
        ")\n",
        "if len(validation_dataset) == 0:\n",
        "    raise ValueError(\"Validation dataset is empty!\")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Define a dummy criterion for validate_model if it expects one (can be ignored in metrics calculation)\n",
        "dummy_criterion = torch.nn.BCELoss()\n",
        "eval_threshold = 0.5 # Threshold for converting scores to binary predictions\n",
        "\n",
        "# --- 4. Validate Zero-Shot CLIP ---\n",
        "print(\"\\n--- Evaluating Zero-Shot CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    _, zs_metrics = validate_zero_shot_clip(\n",
        "        original_clip_model_zs,\n",
        "        validation_loader,\n",
        "        ZERO_SHOT_PROMPTS,\n",
        "        device,\n",
        "        threshold=eval_threshold # You might need to tune this threshold for ZS CLIP\n",
        "    )\n",
        "    print(\"Zero-Shot CLIP Validation Metrics (Frame-Level):\")\n",
        "    print(f\"  Precision: {zs_metrics['precision']:.4f}, Recall: {zs_metrics['recall']:.4f}, F1-Score: {zs_metrics['f1']:.4f}, Accuracy: {zs_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  TN: {zs_metrics['tn']}, FP: {zs_metrics['fp']}, FN: {zs_metrics['fn']}, TP: {zs_metrics['tp']}\")\n",
        "else:\n",
        "    print(\"Skipping Zero-Shot validation as validation_loader is not available or empty.\")\n",
        "\n",
        "# --- 5. Validate Fine-Tuned HL-CLIP Model ---\n",
        "print(\"\\n--- Evaluating Fine-Tuned HL-CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    # Use the `validate_model` function you created earlier for fine-tuned models\n",
        "    _, ft_metrics = validate_model(\n",
        "        fine_tuned_model,\n",
        "        validation_loader,\n",
        "        dummy_criterion, # Loss value is not used for final comparison here\n",
        "        device,\n",
        "        threshold=eval_threshold\n",
        "    )\n",
        "    print(\"Fine-Tuned HL-CLIP Validation Metrics (Frame-Level):\")\n",
        "    print(f\"  Precision: {ft_metrics['precision']:.4f}, Recall: {ft_metrics['recall']:.4f}, F1-Score: {ft_metrics['f1']:.4f}, Accuracy: {ft_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  TN: {ft_metrics['tn']}, FP: {ft_metrics['fp']}, FN: {ft_metrics['fn']}, TP: {ft_metrics['tp']}\")\n",
        "else:\n",
        "    print(\"Skipping Fine-Tuned validation as validation_loader is not available or empty.\")\n",
        "\n",
        "print(\"\\nComparison Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGVdvGV30CFg",
        "outputId": "8c071d89-4bdf-4db1-e942-aa45f6b034b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Zero-Shot CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating Zero-Shot CLIP: 100%|██████████| 2/2 [00:53<00:00, 26.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zs all_preds_flat [31.76775  32.246346 28.456211 30.472013 29.996557 30.593987 28.77274\n",
            " 27.687027 28.491697 29.11325  29.171211 29.434454 29.003923 28.407337\n",
            " 28.412437 29.566784 29.303707 29.644833 29.382765 28.981407 28.456045\n",
            " 28.73938  26.992817 27.359331 28.405855 28.222195 31.565796 31.370588\n",
            " 29.354944 28.996395 23.197866 23.197866 30.845123 29.758429 30.71186\n",
            " 29.675568 28.991556 28.896294 32.415928 29.650026 30.638025 30.343208\n",
            " 29.67817  29.821121 30.473877 30.265873 29.872112 29.26405  30.074356\n",
            " 29.387072 29.825356 29.802698 29.131985 29.367905 29.648687 30.113825\n",
            " 30.613976 29.550934 29.814577 29.93267  29.754381 29.282492 29.249573\n",
            " 29.341824 29.685299 29.339092 29.72827  29.70765  30.735598 30.039925\n",
            " 29.301266 28.982527 29.176945 29.437862 28.596458 28.993315 29.794136\n",
            " 29.791164 28.9961   29.020248 30.030891 29.340263 29.372974 29.577518\n",
            " 29.562239 29.054686 29.13771  29.034243 28.403515 28.655945 28.517519\n",
            " 28.527884 28.903172 29.048607 30.076855 28.72692 ]\n",
            "zs all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Zero-Shot CLIP Validation Metrics (Frame-Level):\n",
            "  Precision: 0.3125, Recall: 1.0000, F1-Score: 0.4762, Accuracy: 0.3125\n",
            "  TN: 0, FP: 66, FN: 0, TP: 30\n",
            "\n",
            "--- Evaluating Fine-Tuned HL-CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:52<00:00, 26.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_preds_flat [0.5157584  0.5082211  0.5388477  0.5304655  0.5437153  0.48685172\n",
            " 0.55484104 0.55144656 0.53762096 0.5552125  0.5509158  0.5429699\n",
            " 0.54940623 0.5425314  0.5434842  0.5193827  0.5379293  0.52262026\n",
            " 0.51678884 0.5212309  0.5130701  0.504389   0.51220626 0.50852317\n",
            " 0.5134127  0.52575195 0.5071466  0.48338506 0.517014   0.53867275\n",
            " 0.5132091  0.5132091  0.47459215 0.49050874 0.5065977  0.5025335\n",
            " 0.49664304 0.48724484 0.5058743  0.510855   0.49401337 0.5158564\n",
            " 0.50771266 0.5127673  0.508235   0.4999202  0.4987277  0.51145107\n",
            " 0.49602687 0.49502146 0.4796038  0.49379668 0.4961827  0.5038079\n",
            " 0.5002023  0.48264557 0.4943828  0.49954075 0.48525488 0.4912602\n",
            " 0.46283615 0.47744036 0.47323698 0.4737816  0.59075683 0.5693961\n",
            " 0.57413036 0.5911803  0.5727741  0.5720326  0.57154346 0.5702821\n",
            " 0.5785874  0.5787152  0.57220626 0.5764902  0.5810822  0.5727958\n",
            " 0.58750015 0.59101397 0.57872945 0.5909166  0.5968977  0.593967\n",
            " 0.5866649  0.59152925 0.5953959  0.5999083  0.5932773  0.58980244\n",
            " 0.5958028  0.60515606 0.5999656  0.59928167 0.59660137 0.5985829 ]\n",
            "all_labels_flat [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Fine-Tuned HL-CLIP Validation Metrics (Frame-Level):\n",
            "  Precision: 0.3836, Recall: 0.9333, F1-Score: 0.5437, Accuracy: 0.5104\n",
            "  TN: 21, FP: 45, FN: 2, TP: 28\n",
            "\n",
            "Comparison Complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xeA6hdDw1Jls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Best Threshold"
      ],
      "metadata": {
        "id": "_kC31OHqXsAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate_model(model, val_loader, criterion, device): # Removed threshold argument here\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions_list = []\n",
        "    all_labels_list = []\n",
        "    batches_processed = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(tqdm(val_loader, desc=\"Validating FT Model\")):\n",
        "            if batch_data is None or batch_data[0] is None: continue\n",
        "            video_frames, labels = batch_data\n",
        "            if video_frames.nelement() == 0: continue\n",
        "\n",
        "            video_frames = video_frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(video_frames)\n",
        "\n",
        "            min_len = min(predictions.shape[1], labels.shape[1])\n",
        "            if min_len == 0: continue\n",
        "            predictions_aligned = predictions[:, :min_len]\n",
        "            labels_aligned = labels[:, :min_len]\n",
        "\n",
        "            if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0: continue\n",
        "\n",
        "            loss = criterion(predictions_aligned, labels_aligned) # Still calculate loss if needed\n",
        "            total_val_loss += loss.item()\n",
        "            batches_processed += 1\n",
        "\n",
        "            all_predictions_list.append(predictions_aligned.cpu().numpy().flatten())\n",
        "            all_labels_list.append(labels_aligned.cpu().numpy().flatten())\n",
        "\n",
        "    if batches_processed == 0:\n",
        "        return 0, np.array([]), np.array([]) # Return empty arrays\n",
        "\n",
        "    avg_val_loss = total_val_loss / batches_processed\n",
        "    all_preds_flat = np.concatenate(all_predictions_list) if all_predictions_list else np.array([])\n",
        "    all_labels_flat = np.concatenate(all_labels_list) if all_labels_list else np.array([])\n",
        "\n",
        "    return avg_val_loss, all_preds_flat, all_labels_flat\n"
      ],
      "metadata": {
        "id": "xu1qur1qXrU4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate_zero_shot_clip(original_clip_model, val_loader, text_prompts, device): # Removed threshold\n",
        "    original_clip_model.eval()\n",
        "    all_predictions_list = []\n",
        "    all_labels_list = []\n",
        "    batches_processed = 0\n",
        "    tokenized_prompts = clip.tokenize(text_prompts).to(device)\n",
        "    text_features = original_clip_model.encode_text(tokenized_prompts)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    logit_scale = original_clip_model.logit_scale.exp()\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(val_loader, desc=\"Validating Zero-Shot CLIP\")):\n",
        "        if batch_data is None or batch_data[0] is None: continue\n",
        "        video_frames, labels = batch_data\n",
        "        if video_frames.nelement() == 0: continue\n",
        "\n",
        "        video_frames = video_frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size, num_clip_frames, C, H, W = video_frames.shape\n",
        "        video_frames_flat = video_frames.view(batch_size * num_clip_frames, C, H, W)\n",
        "        image_features = original_clip_model.encode_image(video_frames_flat)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        similarity = (logit_scale * image_features @ text_features.T)\n",
        "        frame_highlight_scores = similarity.max(dim=1)[0]\n",
        "        frame_highlight_scores = frame_highlight_scores.view(batch_size, num_clip_frames)\n",
        "\n",
        "        min_len = min(frame_highlight_scores.shape[1], labels.shape[1])\n",
        "        if min_len == 0: continue\n",
        "        predictions_aligned = frame_highlight_scores[:, :min_len]\n",
        "        labels_aligned = labels[:, :min_len]\n",
        "\n",
        "        if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0: continue\n",
        "\n",
        "        all_predictions_list.append(predictions_aligned.cpu().numpy().flatten())\n",
        "        all_labels_list.append(labels_aligned.cpu().numpy().flatten())\n",
        "        batches_processed += 1\n",
        "\n",
        "    if batches_processed == 0:\n",
        "        return 0, np.array([]), np.array([]) # Return empty arrays\n",
        "\n",
        "    all_preds_flat = np.concatenate(all_predictions_list) if all_predictions_list else np.array([])\n",
        "    all_labels_flat = np.concatenate(all_labels_list) if all_labels_list else np.array([])\n",
        "\n",
        "    return 0, all_preds_flat, all_labels_flat # Returning dummy loss 0"
      ],
      "metadata": {
        "id": "KEPAmDSaXrbN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def get_metrics_at_threshold(y_true, y_scores, threshold):\n",
        "    \"\"\"Calculates metrics for a given threshold.\"\"\"\n",
        "    if len(y_scores) == 0 or len(y_true) == 0:\n",
        "        return {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0, \"accuracy\": 0, \"threshold\": threshold}\n",
        "\n",
        "    y_pred_binary = (y_scores >= threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "    unique_labels = np.unique(np.concatenate((y_true, y_pred_binary)))\n",
        "    if len(unique_labels) == 1:\n",
        "        if unique_labels[0] == 0: tn, fp, fn, tp = len(y_true), 0,0,0\n",
        "        else: tp, tn, fp, fn = len(y_true), 0,0,0\n",
        "    else:\n",
        "        # Ensure correct ravel order for confusion matrix when labels are specified\n",
        "        cm = confusion_matrix(y_true, y_pred_binary, labels=[0, 1])\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1,\n",
        "            \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp, \"accuracy\": accuracy, \"threshold\": threshold}\n",
        "\n",
        "def find_optimal_threshold_and_metrics(y_true, y_scores, num_steps=100):\n",
        "    \"\"\"\n",
        "    Finds the optimal threshold by maximizing F1-score.\n",
        "    Args:\n",
        "        y_true (np.array): True binary labels.\n",
        "        y_scores (np.array): Predicted scores/probabilities.\n",
        "        num_steps (int): Number of threshold steps to check.\n",
        "    Returns:\n",
        "        dict: Best metrics found at the optimal threshold.\n",
        "    \"\"\"\n",
        "    if len(y_scores) == 0 or len(y_true) == 0:\n",
        "        print(\"Warning: Empty scores or labels array in find_optimal_threshold. Returning zero metrics.\")\n",
        "        return get_metrics_at_threshold(np.array([]), np.array([]), 0.5) # Return default zero metrics\n",
        "\n",
        "    best_f1 = -1\n",
        "    optimal_metrics = get_metrics_at_threshold(y_true, y_scores, 0.5) # Default if no better found\n",
        "\n",
        "    # Define threshold range based on score distribution, or a fixed fine range\n",
        "    min_score = np.min(y_scores) if len(y_scores) > 0 else 0\n",
        "    max_score = np.max(y_scores) if len(y_scores) > 0 else 1\n",
        "\n",
        "    # Adjust thresholds to scan; for CLIP scores, this range might need to be different\n",
        "    # For sigmoid outputs, [0,1] is fine. For raw similarities, it could be wider.\n",
        "    # If max_score is very close to min_score (e.g. all scores are 0.999), linspace might be an issue\n",
        "    if max_score - min_score < 1e-6 and num_steps > 1: # Scores are virtually identical\n",
        "        threshold_range = np.array([min_score]) # Just check this one score\n",
        "    elif min_score >=0 and max_score <=1 and (max_score - min_score > 1e-6) : # typical probability range\n",
        "        threshold_range = np.linspace(0.01, 0.99, num_steps)\n",
        "    else: # Wider range for other types of scores, or if min/max are outside [0,1]\n",
        "        threshold_range = np.linspace(min_score, max_score, num_steps)\n",
        "        if len(threshold_range) == 0 : # if min_score = max_score\n",
        "             threshold_range = np.array([min_score])\n",
        "\n",
        "\n",
        "    print(f\"Searching for optimal threshold in range [{min_score:.4f} - {max_score:.4f}] with {len(threshold_range)} steps...\")\n",
        "\n",
        "    # Also check if the scores have any variance\n",
        "    if np.std(y_scores) < 1e-6 and len(y_scores) > 0: # All scores are practically the same\n",
        "        print(f\"Warning: All predicted scores are nearly identical ({y_scores[0]:.4f}). Threshold optimization might be ineffective.\")\n",
        "        # In this case, we just evaluate at this single score value as a threshold\n",
        "        threshold_range = np.array([y_scores[0] - 1e-5, y_scores[0], y_scores[0] + 1e-5]) # check around this score\n",
        "\n",
        "    for threshold in tqdm(threshold_range, desc=\"Finding Opt. Threshold\", leave=False):\n",
        "        current_metrics = get_metrics_at_threshold(y_true, y_scores, threshold)\n",
        "        if current_metrics[\"f1\"] > best_f1:\n",
        "            best_f1 = current_metrics[\"f1\"]\n",
        "            optimal_metrics = current_metrics\n",
        "        # Optional: if F1 is the same, prefer higher recall or precision based on your needs\n",
        "        elif current_metrics[\"f1\"] == best_f1 and best_f1 > -1 :\n",
        "            # Example: if f1 is same, pick one with higher recall\n",
        "            if current_metrics[\"recall\"] > optimal_metrics[\"recall\"]:\n",
        "                 optimal_metrics = current_metrics\n",
        "\n",
        "\n",
        "    if best_f1 == -1 and len(y_true) > 0: # If no threshold yielded any F1 (e.g. all predictions were one class, and true labels another)\n",
        "        print(\"Warning: Could not find a threshold that yields F1 > -1. Defaulting to 0.5 or min_score.\")\n",
        "        optimal_metrics = get_metrics_at_threshold(y_true, y_scores, np.median(y_scores) if len(y_scores)>0 else 0.5)\n",
        "\n",
        "\n",
        "    print(f\"Optimal threshold found: {optimal_metrics['threshold']:.4f} with F1: {optimal_metrics['f1']:.4f}\")\n",
        "    return optimal_metrics"
      ],
      "metadata": {
        "id": "fXz3cla9X-d1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Zero-shot VS Fine-Tuning"
      ],
      "metadata": {
        "id": "VaE0FZu_fpYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "dummy_criterion = torch.nn.BCELoss() # For validate_model loss calculation (not used for best threshold)\n",
        "\n",
        "# --- 4. Validate Zero-Shot CLIP & Find Optimal Threshold ---\n",
        "print(\"\\n--- Evaluating Zero-Shot CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    _, zs_all_scores, zs_all_labels = validate_zero_shot_clip(\n",
        "        original_clip_model_zs, validation_loader, ZERO_SHOT_PROMPTS, device\n",
        "    )\n",
        "    if len(zs_all_scores) > 0 and len(zs_all_labels) > 0:\n",
        "        print(f\"Zero-Shot: Received {len(zs_all_scores)} scores and {len(zs_all_labels)} labels for threshold optimization.\")\n",
        "        # Plot score distribution (optional but recommended for debugging)\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.hist(zs_all_scores[zs_all_labels==1], bins=50, alpha=0.5, label='ZS Positive Scores')\n",
        "        plt.hist(zs_all_scores[zs_all_labels==0], bins=50, alpha=0.5, label='ZS Negative Scores')\n",
        "        plt.legend()\n",
        "        plt.title(\"Zero-Shot Score Distribution\")\n",
        "        plt.show()\n",
        "\n",
        "        zs_optimal_metrics = find_optimal_threshold_and_metrics(zs_all_labels, zs_all_scores)\n",
        "        print(\"Zero-Shot CLIP Optimal Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Optimal Threshold: {zs_optimal_metrics['threshold']:.4f}\")\n",
        "        print(f\"  Precision: {zs_optimal_metrics['precision']:.4f}, Recall: {zs_optimal_metrics['recall']:.4f}, F1-Score: {zs_optimal_metrics['f1']:.4f}, Accuracy: {zs_optimal_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {zs_optimal_metrics['tn']}, FP: {zs_optimal_metrics['fp']}, FN: {zs_optimal_metrics['fn']}, TP: {zs_optimal_metrics['tp']}\")\n",
        "    else:\n",
        "        print(\"Zero-Shot: No scores returned from validation to find optimal threshold.\")\n",
        "else:\n",
        "    print(\"Skipping Zero-Shot validation.\")\n",
        "\n",
        "# --- 5. Validate Fine-Tuned HL-CLIP Model & Find Optimal Threshold ---\n",
        "print(\"\\n--- Evaluating Fine-Tuned HL-CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    _, ft_all_scores, ft_all_labels = validate_model(\n",
        "        fine_tuned_model, validation_loader, dummy_criterion, device\n",
        "    )\n",
        "    if len(ft_all_scores) > 0 and len(ft_all_labels) > 0:\n",
        "        print(f\"Fine-Tuned: Received {len(ft_all_scores)} scores and {len(ft_all_labels)} labels for threshold optimization.\")\n",
        "        # Plot score distribution (optional)\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.hist(ft_all_scores[ft_all_labels==1], bins=50, alpha=0.5, label='FT Positive Scores')\n",
        "        plt.hist(ft_all_scores[ft_all_labels==0], bins=50, alpha=0.5, label='FT Negative Scores')\n",
        "        plt.legend()\n",
        "        plt.title(\"Fine-Tuned Score Distribution\")\n",
        "        plt.show()\n",
        "\n",
        "        ft_optimal_metrics = find_optimal_threshold_and_metrics(ft_all_labels, ft_all_scores)\n",
        "        print(\"Fine-Tuned HL-CLIP Optimal Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Optimal Threshold: {ft_optimal_metrics['threshold']:.4f}\")\n",
        "        print(f\"  Precision: {ft_optimal_metrics['precision']:.4f}, Recall: {ft_optimal_metrics['recall']:.4f}, F1-Score: {ft_optimal_metrics['f1']:.4f}, Accuracy: {ft_optimal_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {ft_optimal_metrics['tn']}, FP: {ft_optimal_metrics['fp']}, FN: {ft_optimal_metrics['fn']}, TP: {ft_optimal_metrics['tp']}\")\n",
        "    else:\n",
        "        print(\"Fine-Tuned: No scores returned from validation to find optimal threshold.\")\n",
        "else:\n",
        "    print(\"Skipping Fine-Tuned validation.\")\n",
        "\n",
        "print(\"\\nComparison Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8DUrBsawcgLt",
        "outputId": "f3ea5414-5fc6-4505-d6e2-3fbe85df4054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Zero-Shot CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating Zero-Shot CLIP: 100%|██████████| 2/2 [00:48<00:00, 24.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot: Received 96 scores and 96 labels for threshold optimization.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGzCAYAAACPa3XZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQFRJREFUeJzt3Xd4FFX//vF7SUhvBAghEEKkBOmI1ChFetEA0iTSRMBHiogg4IMUKVHpNuyACmINohTFUEUEKaFLkfpIU4RECISQnN8fftkfSxIIsJmQ8H5d11wXe+bMnM/OLMmdaWszxhgBAABYJF9OFwAAAO4uhA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDyCH9OjRQz4+Pjldxl1tzJgxstlslozVoEEDNWjQwP565cqVstls+vLLLy0Zv0ePHipZsqQlYwE3QvjAHc1ms91wGjNmTE6X6eDcuXMaPXq0KlasKG9vbxUsWFBVq1bVM888o2PHjmXr2BMnTtSCBQuy3P/PP//UM888o3LlysnT01NBQUGqWbOmhg0bpnPnzmVfodlg9uzZDp8LDw8PhYSEqFmzZnrttdf0zz//OGWcY8eOacyYMYqPj3fK+pzpTq4NuJprThcAXM/HH3+c6bwxY8bo999/V61atSys6PpSUlJUr149/fbbb+revbsGDBigc+fOaefOnZo3b57atm2rkJCQbBt/4sSJat++vdq0aXPDvn///bfuv/9+JSYm6oknnlC5cuV0+vRpbdu2TTNnztR//vOfXHlk5qWXXlJ4eLhSUlJ04sQJrVy5UoMGDdLUqVO1cOFCVa5c2d535MiRGj58+E2t/9ixYxo7dqxKliypqlWrZnm5H3744abGuRXXq+29995TWlpattcAZAXhA3e0xx9/PMP2999/X7///rsGDBigFi1a3PY4xhhdvHhRnp6et7WeBQsWaMuWLZo7d666dOniMO/ixYu6dOnSba3fmT744AMdOXJEa9euVd26dR3mJSYmys3NzbJazp8/L29vb6esq0WLFrr//vvtr0eMGKHly5erdevWeuSRR7R79277fnZ1dZWra/b+GExKSpKXl5el2zMj+fPnz9Hxgatx2gW5zs6dOzVw4EBVq1ZNkyZNcpiXlpam6dOnq0KFCvLw8FCRIkXUt29fnTlzxqFfyZIl1bp1a33//fe6//775enpqXfeeUeSdODAAXXo0EGBgYHy8vJS7dq1tWjRoizV9vvvv0uSIiMj083z8PCQn59fuvY//vhDbdq0kY+PjwoXLqwhQ4YoNTXVoc/58+f13HPPKTQ0VO7u7oqIiNDkyZN19ZdS22w2nT9/XnPmzLGfeujRo8d1a3VxcVHt2rXTzfPz85OHh4dD2/r169WyZUsVKFBA3t7eqly5smbMmOHQZ/ny5XrwwQfl7e2tgIAARUVFaffu3Q59rlxnsWvXLnXp0kUFChTQAw88YJ//ySefqHr16vL09FRgYKA6d+6so0ePZvo+suKhhx7Siy++qMOHD+uTTz5JV8vVli1bpgceeEABAQHy8fFRRESEXnjhBUn/XqdRo0YNSVLPnj3t23n27NmS/r2uo2LFitq0aZPq1asnLy8v+7LXXvNxRWpqql544QUFBwfL29tbjzzySLr3W7JkyQz35dXrvFFtGV3zkZXPlfTvZ6t///5asGCBKlasKHd3d1WoUEFLly7NeIMDN0D4QK6SlJSkjh07ysXFRfPnz5e7u7vD/L59+2ro0KGKjIzUjBkz1LNnT82dO1fNmjVTSkqKQ989e/boscceU5MmTTRjxgxVrVpVJ0+eVN26dfX999/r6aef1oQJE3Tx4kU98sgjio2NvWF9YWFhkqSPPvoo3Q/wjKSmpqpZs2YqWLCgJk+erPr162vKlCl699137X2MMXrkkUc0bdo0NW/eXFOnTlVERISGDh2qwYMH2/t9/PHHcnd314MPPqiPP/5YH3/8sfr27XvdWlNTU697auuKZcuWqV69etq1a5eeeeYZTZkyRQ0bNtR3331n7/Pjjz+qWbNmOnXqlMaMGaPBgwfr559/VmRkpA4dOpRunR06dFBSUpImTpyo3r17S5ImTJigbt26qUyZMpo6daoGDRqkuLg41atXT2fPnr1hndfTtWtXSdc//bFz5061bt1aycnJeumllzRlyhQ98sgjWrt2rSTp3nvv1UsvvSRJ6tOnj30716tXz76O06dPq0WLFqpataqmT5+uhg0bXreuCRMmaNGiRRo2bJgGDhyoZcuWqXHjxrpw4cJNvb+s1Ha1rH6urvjpp5/09NNPq3Pnznr11Vd18eJFPfroozp9+vRN1QlIkgyQizzxxBNGkpkzZ066eWvWrDGSzNy5cx3aly5dmq49LCzMSDJLly516Dto0CAjyaxZs8be9s8//5jw8HBTsmRJk5qaet36kpKSTEREhJFkwsLCTI8ePcwHH3xgTp48ma5v9+7djSTz0ksvObRXq1bNVK9e3f56wYIFRpIZP368Q7/27dsbm81m9u/fb2/z9vY23bt3v26NV5w4ccIULlzYSDLlypUzTz31lJk3b545e/asQ7/Lly+b8PBwExYWZs6cOeMwLy0tzf7vqlWrmqCgIHP69Gl729atW02+fPlMt27d7G2jR482ksxjjz3msK5Dhw4ZFxcXM2HCBIf27du3G1dX13Tt15o1a5aRZH799ddM+/j7+5tq1aqlq+WKadOmGUnmzz//zHQdv/76q5FkZs2alW5e/fr1jSTz9ttvZzivfv369tcrVqwwkkyxYsVMYmKivf3zzz83ksyMGTPsbWFhYRnu12vXeb3aunfvbsLCwuyvb+ZzJcm4ubk5tG3dutVIMq+//nq6sYAb4cgHco158+bpww8/VNeuXdWtW7d087/44gv5+/urSZMm+uuvv+xT9erV5ePjoxUrVjj0Dw8PV7NmzRzaFi9erJo1azqcBvDx8VGfPn106NAh7dq167o1enp6av369Ro6dKikf+/A6NWrl4oWLaoBAwYoOTk53TJPPfWUw+sHH3xQBw4ccKjJxcVFAwcOdOj33HPPyRijJUuWXLemzBQpUkRbt27VU089pTNnzujtt99Wly5dFBQUpHHjxtmP3GzZskUHDx7UoEGDFBAQ4LCOK6csjh8/rvj4ePXo0UOBgYH2+ZUrV1aTJk20ePHiG77vr7/+WmlpaerYsaPD/gsODlaZMmXS7b9b4ePjc927Xq68v2+++eaWL850d3dXz549s9y/W7du8vX1tb9u3769ihYtmuE2c6ab/Vw1btxYpUqVsr+uXLmy/Pz8HD6rQFYRPpAr7Nu3T0899ZTKli2rt956K9M+CQkJCgoKUuHChR2mc+fO6dSpUw79w8PD063j8OHDioiISNd+77332udL/94pcuLECfuUkJBg7+vv769XX31Vhw4d0qFDh/TBBx8oIiJCb7zxhsaNG+ewXg8PDxUuXNihrUCBAg7XqBw+fFghISEOv6AyqulWFC1aVDNnztTx48e1Z88evfbaaypcuLBGjRqlDz74QNL/v46lYsWKma7nSg2Zbbu//vpL58+fd2i/dvvv27dPxhiVKVMm3f7bvXt3uv13K86dO5duO16tU6dOioyM1JNPPqkiRYqoc+fO+vzzz28qiBQrVuymLi4tU6aMw2ubzabSpUtneKrKmW72c1WiRIl067j2swpkFXe74I6XnJysTp066dKlS5o/f36mt3+mpaUpKChIc+fOzXD+tb/kb+fOlnbt2mnVqlX21927d7df2He1sLAwPfHEE2rbtq3uuecezZ07V+PHj7fPd3FxueUanMlms6ls2bIqW7asWrVqpTJlymju3Ll68skns23Ma7d/WlqabDablixZkuF2ud3bfv/3v/8pISFBpUuXvm5Nq1ev1ooVK7Ro0SItXbpUn332mR566CH98MMPWdpft3vHVEYyexBaamqqZZ+hzMYxWbi2CbgW4QN3vCFDhmjLli2aMWOGqlWrlmm/UqVK6ccff1RkZOQt/wIICwvTnj170rX/9ttv9vmSNGXKFIe/+G707I4CBQqoVKlS2rFjxy3V9OOPP+qff/5x+Cv12pqkzH9J3Yx77rlHBQoU0PHjxyXJfqh9x44daty4caY1Ssp02xUqVOiGt9KWKlVKxhiFh4erbNmyt/MWMnTlwtprT7VdK1++fGrUqJEaNWqkqVOnauLEifrvf/+rFStWqHHjxk5/Iuq+ffscXhtjtH//fofnkRQoUCDDC24PHz6se+65x/76Zmq7mc8V4GycdsEdLTY2Vm+88YYeeeSRdOemr9WxY0elpqamO7UhSZcvX87S3RItW7bUhg0btG7dOnvb+fPn9e6776pkyZIqX768JKl69epq3LixfbrSvnXrVv3111/p1nv48GHt2rUrw9MSWakpNTVVb7zxhkP7tGnTZLPZHJ5z4u3tneW7QtavX5/uVIgkbdiwQadPn7bXet999yk8PFzTp09Pt+4rf/UWLVpUVatW1Zw5cxz67NixQz/88INatmx5w3ratWsnFxcXjR07Nt1f08aY27qrYvny5Ro3bpzCw8MVHR2dab+///47XduVh3VduV7nSoi63btvrvjoo48crkP58ssvdfz4cYf9WqpUKf3yyy8Oz4n57rvv0t2SezO13cznCnA2jnzgjnX8+HH16tVLLi4uatSokcPzGa5WqlQp1alTR/Xr11ffvn0VExOj+Ph4NW3aVPnz59e+ffv0xRdfaMaMGWrfvv11xxw+fLg+/fRTtWjRQgMHDlRgYKDmzJmjgwcP6quvvlK+fNfP68uWLdPo0aP1yCOPqHbt2vLx8dGBAwf04YcfKjk5+ZYeBf/www+rYcOG+u9//6tDhw6pSpUq+uGHH/TNN99o0KBBDhcBVq9eXT/++KOmTp2qkJAQhYeHZ/oE2I8//lhz585V27ZtVb16dbm5uWn37t368MMP5eHhYX8+Rb58+TRz5kw9/PDDqlq1qnr27KmiRYvqt99+086dO/X9999LkiZNmqQWLVqoTp066tWrly5cuKDXX39d/v7+WXrfpUqV0vjx4zVixAgdOnRIbdq0ka+vrw4ePKjY2Fj16dNHQ4YMueF6lixZot9++02XL1/WyZMntXz5ci1btkxhYWFauHBhuueXXO2ll17S6tWr1apVK4WFhenUqVN66623VLx4cftFyKVKlVJAQIDefvtt+fr6ytvbW7Vq1crwGqKsCAwM1AMPPKCePXvq5MmTmj59ukqXLm2//ViSnnzySX355Zdq3ry5OnbsqN9//12ffPKJw76/2dpu5nMFOF1O3WYD3MiVWxFvNF17C+K7775rqlevbjw9PY2vr6+pVKmSef75582xY8fsfcLCwkyrVq0yHPf333837du3NwEBAcbDw8PUrFnTfPfdd1mq+cCBA2bUqFGmdu3aJigoyLi6uprChQubVq1ameXLlzv07d69u/H29k63jmtv/zTm39t9n332WRMSEmLy589vypQpYyZNmuRwq6sxxvz222+mXr16xtPTM8Ntc7Vt27aZoUOHmvvuu88EBgYaV1dXU7RoUdOhQwezefPmdP1/+ukn06RJE+Pr62u8vb1N5cqV091m+eOPP5rIyEjj6elp/Pz8zMMPP2x27dqV4fvL7HbWr776yjzwwAPG29vbeHt7m3Llypl+/fqZPXv2ZPpejPn/t9pemdzc3ExwcLBp0qSJmTFjhsPtrNfWckVcXJyJiooyISEhxs3NzYSEhJjHHnvM7N2712G5b775xpQvX964uro63Npav359U6FChQzry+xW208//dSMGDHCBAUFGU9PT9OqVStz+PDhdMtPmTLFFCtWzLi7u5vIyEizcePGdOu8Xm3X3mprTNY/V5JMv3790tWU2S3AwI3YjOFqIQAAYB2u+QAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsNQd95CxtLQ0HTt2TL6+vk5/jDEAAMgexhj9888/CgkJueEDGe+48HHs2DGFhobmdBkAAOAWHD16VMWLF79unzsufFz5gqOjR4/Kz88vh6sBAABZkZiYqNDQUIcvKszMHRc+rpxq8fPzI3wAAJDLZOWSCS44BQAAliJ8AAAASxE+AACApe64az6ywhijy5cvKzU1NadLAZzKxcVFrq6u3GYOIE/LdeHj0qVLOn78uJKSknK6FCBbeHl5qWjRonJzc8vpUgAgW+Sq8JGWlqaDBw/KxcVFISEhcnNz4y9E5BnGGF26dEl//vmnDh48qDJlytzwQT0AkBvlqvBx6dIlpaWlKTQ0VF5eXjldDuB0np6eyp8/vw4fPqxLly7Jw8Mjp0sCAKfLlX9W8dcg8jI+3wDyOn7KAQAASxE+AACApXLVNR/XM23ZXkvHe7ZJWUvHyyk9evTQ2bNntWDBgkz7rFy5Ug0bNtSZM2cUEBBgWW0AgNyJIx8WWLlypWw2W6ZTw4YN7X1jY2NVu3Zt+fv7y9fXVxUqVNCgQYOuu/6r1+Xv76/IyEgtX77cKbXPmDFDs2fPtr9u0KBBunrq1q2r48ePy9/f3yljZuZWtg0A4M5D+LDAlV/O107vvPOObDabnn76aUlSXFycOnXqpEcffVQbNmzQpk2bNGHCBKWkpNxwjFmzZun48eNau3atChUqpNatW+vAgQO3Xbu/v/8Nj2a4ubkpODg4W297vp1tc6tSU1OVlpaWbesHgLsV4cMCV345Xz2dOXNGQ4YM0QsvvKAOHTpIkr799ltFRkZq6NChioiIUNmyZdWmTRu9+eabNxwjICBAwcHBqlixombOnKkLFy5o2bJlkqRVq1apZs2acnd3V9GiRTV8+HBdvnzZvuyXX36pSpUqydPTUwULFlTjxo11/vx5Sf+edmnTpo3936tWrdKMGTPsR1oOHTpkP7Jz9uxZJSYmytPTU0uWLHGoLzY2Vr6+vvaHwx09elQdO3ZUQECAAgMDFRUVpUOHDmX6/rK6bb799lvVqFFDHh4eKlSokNq2bWufd+bMGXXr1k0FChSQl5eXWrRooX379tnnz549WwEBAVq4cKHKly8vd3d3HTlyRMnJyRoyZIiKFSsmb29v1apVSytXrrQvd/jwYT388MMqUKCAvL29VaFCBS1evPiG+wwA7laEjxxw9uxZRUVFqUGDBho3bpy9PTg4WDt37tSOHTtua/2enp6S/n0uyh9//KGWLVuqRo0a2rp1q2bOnKkPPvhA48ePlyQdP35cjz32mJ544gnt3r1bK1euVLt27WSMSbfeGTNmqE6dOurdu7f96E1oaKhDHz8/P7Vu3Vrz5s1zaJ87d67atGkjLy8vpaSkqFmzZvL19dWaNWu0du1a+fj4qHnz5rp06VKG7ykr22bRokVq27atWrZsqS1btiguLk41a9a0z+/Ro4c2btyohQsXat26dTLGqGXLlg5HT5KSkvTKK6/o/fff186dOxUUFKT+/ftr3bp1mj9/vrZt26YOHTqoefPm9uDSr18/JScna/Xq1dq+fbteeeUV+fj4XG8XAdZYEXPjCcgBeeaC09wiLS1NXbp0kaurq+bOnetwqmLAgAFas2aNKlWqpLCwMNWuXVtNmzZVdHS03N3ds7T+pKQkjRw5Ui4uLqpfv77eeusthYaG6o033pDNZlO5cuV07NgxDRs2TKNGjdLx48d1+fJltWvXTmFhYZKkSpUqZbhuf39/ubm5ycvLS8HBwZnWEB0dra5duyopKUleXl5KTEzUokWLFBsbK0n67LPPlJaWpvfff9/+/mfNmqWAgACtXLlSTZs2TbfOrGybCRMmqHPnzho7dqx9uSpVqkiS9u3bp4ULF2rt2rWqW7eupH8DUWhoqBYsWGA/+pSSkqK33nrLvtyRI0c0a9YsHTlyRCEhIZKkIUOGaOnSpZo1a5YmTpyoI0eO6NFHH7Vvt3vuuScruwoA7loc+bDYCy+8oHXr1umbb76Rr6+vwzxvb28tWrRI+/fv18iRI+Xj46PnnntONWvWvOF32Tz22GPy8fGRr6+vvvrqK33wwQeqXLmydu/erTp16jiEnMjISJ07d07/+9//VKVKFTVq1EiVKlVShw4d9N577+nMmTO39R5btmyp/Pnza+HChZKkr776Sn5+fmrcuLEkaevWrdq/f798fX3l4+MjHx8fBQYG6uLFi/r9998zXGdWtk18fLwaNWqU4fK7d++Wq6uratWqZW8rWLCgIiIitHv3bnubm5ubKleubH+9fft2paamqmzZsvZafXx8tGrVKnutAwcO1Pjx4xUZGanRo0dr27Ztt7H1ACDvI3xYaP78+Zo8ebLmz5+vMmXKZNqvVKlSevLJJ/X+++9r8+bN2rVrlz777LPrrnvatGmKj4/XiRMndOLECXXv3j1LNbm4uGjZsmVasmSJypcvr9dff10RERE6ePDgTb23q7m5ual9+/b2Uy/z5s1Tp06d5Or674G2c+fOqXr16oqPj3eY9u7dqy5dulx33dfbNldON90OT09Ph6B27tw5ubi4aNOmTQ617t69WzNmzJAkPfnkkzpw4IC6du2q7du36/7779frr79+27UAQF5F+LBIfHy8evXqpZdfflnNmjXL8nIlS5aUl5eX/QLQzAQHB6t06dIqXLiwQ/u9995rv77hirVr18rX11fFixeX9O+tupGRkRo7dqy2bNkiNzc3+ymSa7m5uSk1NfWGdUdHR2vp0qXauXOnli9frujoaPu8++67T/v27VNQUJBKly7tMN3M7brXbpvKlSsrLi4uw7733nuvLl++rPXr19vbTp8+rT179qh8+fKZjlGtWjWlpqbq1KlT6Wq9+tRTaGionnrqKX399dd67rnn9N5772X5fQDA3YZrPizw119/qU2bNmrQoIEef/xxnThxwmG+i4uLChcurDFjxigpKUktW7ZUWFiYzp49q9dee00pKSlq0qTJLY399NNPa/r06RowYID69++vPXv2aPTo0Ro8eLDy5cun9evXKy4uTk2bNlVQUJDWr1+vP//8U/fee2+G6ytZsqTWr1+vQ4cO2U+XZKRevXoKDg5WdHS0wsPDHU53REdHa9KkSYqKitJLL72k4sWL6/Dhw/r666/1/PPP20PR1bKybUaPHq1GjRqpVKlS6ty5sy5fvqzFixdr2LBhKlOmjKKiotS7d2+988478vX11fDhw1WsWDFFRUVluv3Kli2r6OhodevWTVOmTFG1atX0559/Ki4uTpUrV1arVq00aNAgtWjRQmXLltWZM2e0YsWKTLcfACAPhY87+YmjixYt0uHDh3X48GEVLVo03fywsDAdOnRI9evX15tvvqlu3brp5MmTKlCggKpVq6YffvhBERERtzR2sWLFtHjxYg0dOlRVqlRRYGCgevXqpZEjR0r69+6U1atXa/r06UpMTFRYWJimTJmiFi1aZLi+IUOGqHv37ipfvrwuXLiQ6ekZm82mxx57TK+++qpGjRrlMM/Ly0urV6/WsGHD1K5dO/3zzz8qVqyYGjVqJD8/vwzXl5Vt06BBA33xxRcaN26cXn75Zfn5+alevXr2dcyaNUvPPPOMWrdurUuXLqlevXpavHix8ufPf91tOGvWLI0fP17PPfec/vjjDxUqVEi1a9dW69atJf37PJB+/frpf//7n/z8/NS8eXNNmzbtuusEgLuZzWR0T2UOSkxMlL+/vxISEtL9Irp48aIOHjyo8PBwvmoceRafczhNVm6lbTgi++vAXeF6v7+vxTUfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBSeebx6ll6kp8z8VTAm7Jy5Uo1bNhQZ86cUUBAQE6XAwDIQRz5sMDKlStls9kynRo2bGjvGxsbq9q1a8vf31++vr6qUKGCBg0adN3122w2eXh46PDhww7tbdq0UY8ePbLhHV1fgwYN0tVct25dHT9+/Ka+tfZW3Mr2AwBYi/BhgSu/eK+d3nnnHdlsNj399NOSpLi4OHXq1EmPPvqoNmzYoE2bNmnChAlKSUm54Rg2my3dF7jdSdzc3BQcHCybzZZtY9zO9rtVqampSktLy7b1A0BeRPiwwJVfvFdPZ86c0ZAhQ/TCCy+oQ4cOkqRvv/1WkZGRGjp0qCIiIlS2bFm1adNGb7755g3H6N+/vz755BPt2LEj0z5paWmKiYlReHi4PD09VaVKFX355ZcOfRYuXKgyZcrIw8NDDRs21Jw5c2Sz2XT27FlJ0unTp/XYY4+pWLFi8vLyUqVKlfTpp5/al+/Ro4dWrVqlGTNm2I/sHDp0yH705+zZs0pMTJSnp6eWLFniMHZsbKx8fX2VlJQkSTp69Kg6duyogIAABQYGKioqSocOHcr0/WV1+3377beqUaOGPDw8VKhQIbVt29Y+78yZM+rWrZsKFCggLy8vtWjRQvv27bPPnz17tgICArRw4UKVL19e7u7uOnLkiJKTkzVkyBAVK1ZM3t7eqlWrllauXGlf7vDhw3r44YdVoEABeXt7q0KFClq8eHGm7wUA8jLCRw44e/asoqKi1KBBA40bN87eHhwcrJ07d143QGQmMjJSrVu31vDhwzPtExMTo48++khvv/22du7cqWeffVaPP/64Vq1aJUk6ePCg2rdvrzZt2mjr1q3q27ev/vvf/zqs4+LFi6pevboWLVqkHTt2qE+fPuratas2bNggSZoxY4bq1Kmj3r1724/whIaGOqzDz89PrVu31rx58xza586dqzZt2sjLy0spKSlq1qyZfH19tWbNGq1du1Y+Pj5q3ry5Ll26lOH7y8r2W7Rokdq2bauWLVtqy5YtiouLU82aNe3ze/TooY0bN2rhwoVat26djDFq2bKlw9GTpKQkvfLKK3r//fe1c+dOBQUFqX///lq3bp3mz5+vbdu2qUOHDmrevLk9uPTr10/JyclavXq1tm/frldeeUU+Pj6Z1gkAeVneueA0l0hLS1OXLl3k6uqquXPnOpyGGDBggNasWaNKlSopLCxMtWvXVtOmTRUdHS13d/cbrjsmJkaVK1fWmjVr9OCDDzrMS05O1sSJE/Xjjz+qTp06kqR77rlHP/30k9555x3Vr19f77zzjiIiIjRp0iRJUkREhHbs2KEJEybY11OsWDENGTLEoebvv/9en3/+uWrWrCl/f3+5ubnJy8tLwcHBmdYaHR2trl27KikpSV5eXkpMTNSiRYsUGxsrSfrss8+Ulpam999/376NZs2apYCAAK1cuVJNmzZNt86sbL8JEyaoc+fOGjt2rH25KlWqSJL27dunhQsXau3atapbt66kfwNRaGioFixYYD9ClZKSorfeesu+3JEjRzRr1iwdOXJEISEhkqQhQ4Zo6dKlmjVrliZOnKgjR47o0UcfVaVKlezbHgDuVhz5sNgLL7ygdevW6ZtvvpGvr6/DPG9vby1atEj79+/XyJEj5ePjo+eee041a9a0n4q4nvLly6tbt24ZHv3Yv3+/kpKS1KRJE/n4+Ninjz76SL///rskac+ePapRo4bDclcfFZD+vcZh3LhxqlSpkgIDA+Xj46Pvv/9eR44cuant0LJlS+XPn18LFy6UJH311Vfy8/NT48aNJUlbt27V/v375evra681MDBQFy9etNd7raxsv/j4eDVq1CjD5Xfv3i1XV1fVqlXL3lawYEFFRERo9+7d9jY3NzdVrlzZ/nr79u1KTU1V2bJlHbbtqlWr7LUOHDhQ48ePV2RkpEaPHq1t27bd1PYCgLzkpsPH6tWr9fDDDyskJEQ2m00LFiywz0tJSdGwYcNUqVIleXt7KyQkRN26ddOxY8ecWXOuNX/+fE2ePFnz589XmTJlMu1XqlQpPfnkk3r//fe1efNm7dq1S5999lmWxhg7dqw2b97ssF8k6dy5c5L+Pe0QHx9vn3bt2pXuuo/rmTRpkmbMmKFhw4ZpxYoVio+PV7NmzTI9FZIZNzc3tW/f3n7qZd68eerUqZNcXV3t9VavXt2h1vj4eO3du1ddunS57rqvt/08PT1vqs6MeHp6OhyxOnfunFxcXLRp0yaHWnfv3q0ZM2ZIkp588kkdOHBAXbt21fbt23X//ffr9ddfv+1aACA3uunwcf78eVWpUiXDiyCTkpK0efNmvfjii9q8ebO+/vpr7dmzR4888ohTis3N4uPj1atXL7388stq1qxZlpcrWbKkvLy8dP78+Sz1Dw0NVf/+/fXCCy8oNTXV3n71xZGlS5d2mK5ckxEREaGNGzc6rO/XX391eL127VpFRUXp8ccfV5UqVXTPPfdo7969Dn3c3Nwcxs5MdHS0li5dqp07d2r58uWKjo62z7vvvvu0b98+BQUFpav3Zm7XvXb7Va5cWXFxcRn2vffee3X58mWtX7/e3nb69Gnt2bNH5cuXz3SMatWqKTU1VadOnUpX69WnnkJDQ/XUU0/p66+/1nPPPaf33nsvy+8DAPKSm77mo0WLFmrRokWG8/z9/bVs2TKHtjfeeEM1a9bUkSNHVKJEiXTLJCcnKzk52f46MTHxZku64/31119q06aNGjRooMcff1wnTpxwmO/i4qLChQtrzJgxSkpKUsuWLRUWFqazZ8/qtddeU0pKipo0aZLl8UaMGKH33ntPBw8eVKdOnSRJvr6+GjJkiJ599lmlpaXpgQceUEJCgtauXSs/Pz91795dffv21dSpUzVs2DD16tVL8fHxmj17tiTZ/9IvU6aMvvzyS/38888qUKCApk6dqpMnTzr8ci5ZsqTWr1+vQ4cO2U+XZKRevXoKDg5WdHS0wsPDHU53REdHa9KkSYqKitJLL72k4sWL6/Dhw/r666/1/PPPq3jx4unWl5XtN3r0aDVq1EilSpVS586ddfnyZS1evFjDhg1TmTJlFBUVpd69e+udd96Rr6+vhg8frmLFiikqKirT7V22bFlFR0erW7dumjJliqpVq6Y///xTcXFxqly5slq1aqVBgwapRYsWKlu2rM6cOaMVK1bo3nvvzfI+BYA8xdwGSSY2Nva6fZYtW2ZsNptJSEjIcP7o0aONpHRTRv0vXLhgdu3aZS5cuHA7ZVtu9uzZGb7HK1NYWJgxxpjly5ebRx991ISGhho3NzdTpEgR07x5c7NmzZrrrj+j/TBx4kQjyXTv3t3elpaWZqZPn24iIiJM/vz5TeHChU2zZs3MqlWr7H2++eYbU7p0aePu7m4aNGhgZs6caSTZt/np06dNVFSU8fHxMUFBQWbkyJGmW7duJioqyr6OPXv2mNq1axtPT08jyRw8eNCsWLHCSDJnzpxxqPP55583ksyoUaPSva/jx4+bbt26mUKFChl3d3dzzz33mN69e2f6Wcrq9vvqq69M1apVjZubmylUqJBp166dfd7ff/9tunbtavz9/Y2np6dp1qyZ2bt3r33+rFmzjL+/f7qxL126ZEaNGmVKlixp8ufPb4oWLWratm1rtm3bZowxpn///qZUqVLG3d3dFC5c2HTt2tX89ddfGb6P3Po5xx1o+cQbT4CTJCQkZPr7+1o2Y4y51eBis9kUGxurNm3aZDj/4sWLioyMVLly5TR37twM+2R05CM0NFQJCQny8/NLt76DBw8qPDxcHh4et1o2bsKECRP09ttv6+jRozldyl2DzzmcJitfO8FXRcBJEhMT5e/vn+Hv72tl2622KSkp6tixo4wxmjlzZqb93N3ds3QbKazx1ltvqUaNGipYsKDWrl2rSZMmqX///jldFgAgD8mW8HEleBw+fFjLly+/YQLCnWPfvn0aP368/v77b5UoUULPPfecRozgLyMAgPM4PXxcCR779u3TihUrVLBgQWcPgWw0bdo0TZs2LafLAADkYTcdPs6dO6f9+/fbXx88eFDx8fEKDAxU0aJF1b59e23evFnfffedUlNT7Xd2BAYGys3NzXmVAwCAXOmmw8fGjRsdvgJ+8ODBkqTu3btrzJgx9idWVq1a1WG5FStWqEGDBrde6VVu4xpZ4I7H5xtAXnfT4aNBgwbX/eGYnT848+fPL+nfh5k540mVwJ3oyqPgr3zeASCvyVVfLOfi4qKAgACdOnVKkuTl5eXwmGsgNzPGKCkpSadOnVJAQIBcXFxyuiQAyBa5KnxIsj+u+koAAfKagICA634jMADkdrkufNhsNhUtWlRBQUFKSUnJ6XIAp8qfPz9HPADkebkufFzh4uLCD2kAAHKhm/5WWwAAgNtB+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACw1E2Hj9WrV+vhhx9WSEiIbDabFixY4DDfGKNRo0apaNGi8vT0VOPGjbVv3z5n1QsAAHK5mw4f58+fV5UqVfTmm29mOP/VV1/Va6+9prffflvr16+Xt7e3mjVrposXL952sQAAIPdzvdkFWrRooRYtWmQ4zxij6dOna+TIkYqKipIkffTRRypSpIgWLFigzp073161AAAg13PqNR8HDx7UiRMn1LhxY3ubv7+/atWqpXXr1mW4THJyshITEx0mAACQdzk1fJw4cUKSVKRIEYf2IkWK2OddKyYmRv7+/vYpNDTUmSUBAIA7TI7f7TJixAglJCTYp6NHj+Z0SQAAIBs5NXwEBwdLkk6ePOnQfvLkSfu8a7m7u8vPz89hAgAAeZdTw0d4eLiCg4MVFxdnb0tMTNT69etVp04dZw4FAAByqZu+2+XcuXPav3+//fXBgwcVHx+vwMBAlShRQoMGDdL48eNVpkwZhYeH68UXX1RISIjatGnjzLoBAEAuddPhY+PGjWrYsKH99eDBgyVJ3bt31+zZs/X888/r/Pnz6tOnj86ePasHHnhAS5culYeHh/OqBgAAuZbNGGNyuoirJSYmyt/fXwkJCVz/AQC3Y0XMjfs0HJH9deCucDO/v3P8bhcAAHB3IXwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUq45XQAA4CorYrLWr+GI7K0DyEYc+QAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALCU08NHamqqXnzxRYWHh8vT01OlSpXSuHHjZIxx9lAAACAXcnX2Cl955RXNnDlTc+bMUYUKFbRx40b17NlT/v7+GjhwoLOHAwAAuYzTw8fPP/+sqKgotWrVSpJUsmRJffrpp9qwYYOzhwIAALmQ00+71K1bV3Fxcdq7d68kaevWrfrpp5/UokWLDPsnJycrMTHRYQIAAHmX0498DB8+XImJiSpXrpxcXFyUmpqqCRMmKDo6OsP+MTExGjt2rLPLAABrrYi5cZ+GI7K/DiAXcPqRj88//1xz587VvHnztHnzZs2ZM0eTJ0/WnDlzMuw/YsQIJSQk2KejR486uyQAAHAHcfqRj6FDh2r48OHq3LmzJKlSpUo6fPiwYmJi1L1793T93d3d5e7u7uwyAADAHcrpRz6SkpKUL5/jal1cXJSWlubsoQAAQC7k9CMfDz/8sCZMmKASJUqoQoUK2rJli6ZOnaonnnjC2UMBAIBcyOnh4/XXX9eLL76op59+WqdOnVJISIj69u2rUaNGOXsoAACQCzk9fPj6+mr69OmaPn26s1cNAADyAL7bBQAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJbKlvDxxx9/6PHHH1fBggXl6empSpUqaePGjdkxFAAAyGVcnb3CM2fOKDIyUg0bNtSSJUtUuHBh7du3TwUKFHD2UAAAIBdyevh45ZVXFBoaqlmzZtnbwsPDnT0MAADIpZx+2mXhwoW6//771aFDBwUFBalatWp67733Mu2fnJysxMREhwkAAORdTg8fBw4c0MyZM1WmTBl9//33+s9//qOBAwdqzpw5GfaPiYmRv7+/fQoNDXV2SQAA4A7i9PCRlpam++67TxMnTlS1atXUp08f9e7dW2+//XaG/UeMGKGEhAT7dPToUWeXBAAA7iBODx9FixZV+fLlHdruvfdeHTlyJMP+7u7u8vPzc5gAAEDe5fTwERkZqT179ji07d27V2FhYc4eCgAA5EJODx/PPvusfvnlF02cOFH79+/XvHnz9O6776pfv37OHgoAAORCTg8fNWrUUGxsrD799FNVrFhR48aN0/Tp0xUdHe3soQAAQC7k9Od8SFLr1q3VunXr7Fg1AADI5fhuFwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACzlmtMFAEB2mbZsb7q2Z5uUzYFK/s+KmDtzXc4Yq+GI7K8DeQZHPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACyV7eHj5Zdfls1m06BBg7J7KAAAkAtka/j49ddf9c4776hy5crZOQwAAMhFsi18nDt3TtHR0XrvvfdUoECB7BoGAADkMtkWPvr166dWrVqpcePG1+2XnJysxMREhwkAAORdrtmx0vnz52vz5s369ddfb9g3JiZGY8eOzY4yAAC5zYqYG/dpOCL760C2cvqRj6NHj+qZZ57R3Llz5eHhccP+I0aMUEJCgn06evSos0sCAAB3EKcf+di0aZNOnTql++67z96Wmpqq1atX64033lBycrJcXFzs89zd3eXu7u7sMgAAwB3K6eGjUaNG2r59u0Nbz549Va5cOQ0bNswheAAAgLuP08OHr6+vKlas6NDm7e2tggULpmsHAAB3H55wCgAALJUtd7tca+XKlVYMAwAAcgGOfAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGAp15wuAACyS+0j76ZvXFHw5lfUcMTtFwPAjiMfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAlnJ6+IiJiVGNGjXk6+uroKAgtWnTRnv27HH2MAAAIJdyevhYtWqV+vXrp19++UXLli1TSkqKmjZtqvPnzzt7KAAAkAu5OnuFS5cudXg9e/ZsBQUFadOmTapXr56zhwMAALmM08PHtRISEiRJgYGBGc5PTk5WcnKy/XViYmJ2lwQAAHJQtoaPtLQ0DRo0SJGRkapYsWKGfWJiYjR27NjsLMPRipgb92k4IvvrAHDHW3fg9L//ODDE3lbnnoI3t+wNZLa+jJbP6tg3JSs/E521Hn624v9k690u/fr1044dOzR//vxM+4wYMUIJCQn26ejRo9lZEgAAyGHZduSjf//++u6777R69WoVL148037u7u5yd3fPrjIAAMAdxunhwxijAQMGKDY2VitXrlR4eLizhwAAALmY08NHv379NG/ePH3zzTfy9fXViRMnJEn+/v7y9PR09nAAACCXcfo1HzNnzlRCQoIaNGigokWL2qfPPvvM2UMBAIBcKFtOuwAAAGSG73YBAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUq45XQAA55u2bG+6tmeblM2RZW9m+ay6nRrXHTidYXudewrecj2ZrdPZy97OOFl9f9mxfexWxNxwnCyNe9V6MtVwRFarytTNfM6y2vd2PrtZHbf2kXdvvL+csH1uB0c+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALJVt4ePNN99UyZIl5eHhoVq1amnDhg3ZNRQAAMhFsiV8fPbZZxo8eLBGjx6tzZs3q0qVKmrWrJlOnTqVHcMBAIBcJFvCx9SpU9W7d2/17NlT5cuX19tvvy0vLy99+OGH2TEcAADIRVydvcJLly5p06ZNGjFihL0tX758aty4sdatW5euf3JyspKTk+2vExISJEmJiYnOLu1f5y/euE92jQ1Y5OL5c+nasvp/ytnL3szyWZXVGs9fSE7XlpnEDH42ZLR8VvvdaTKqOyOZvZesLp9VWd1mtzyuEz5zN/N/Iat9b+f/V1bHPX8h+cbbLRt+z115H8aYG3c2TvbHH38YSebnn392aB86dKipWbNmuv6jR482kpiYmJiYmJjywHT06NEbZgWnH/m4WSNGjNDgwYPtr9PS0vT333+rYMGCstlsOViZtRITExUaGqqjR4/Kz88vp8u5a7Ef7gzshzsD+yHn5aZ9YIzRP//8o5CQkBv2dXr4KFSokFxcXHTy5EmH9pMnTyo4ODhdf3d3d7m7uzu0BQQEOLusXMPPz++O/4DdDdgPdwb2w52B/ZDzcss+8Pf3z1I/p19w6ubmpurVqysuLs7elpaWpri4ONWpU8fZwwEAgFwmW067DB48WN27d9f999+vmjVravr06Tp//rx69uyZHcMBAIBcJFvCR6dOnfTnn39q1KhROnHihKpWraqlS5eqSJEi2TFcnuDu7q7Ro0enOwUFa7Ef7gzshzsD+yHn5dV9YDMmK/fEAAAAOAff7QIAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwvFxMSoRo0a8vX1VVBQkNq0aaM9e/Zk2NcYoxYtWshms2nBggXWFprHZXU/rFu3Tg899JC8vb3l5+enevXq6cKFCzlQcd6Ulf1w4sQJde3aVcHBwfL29tZ9992nr776KocqzptmzpypypUr25+gWadOHS1ZssQ+/+LFi+rXr58KFiwoHx8fPfroo+meYI3bd7398Pfff2vAgAGKiIiQp6enSpQooYEDB9q/iDU3InxYaNWqVerXr59++eUXLVu2TCkpKWratKnOnz+fru/06dPvqu+2sVJW9sO6devUvHlzNW3aVBs2bNCvv/6q/v37K18+/ss4S1b2Q7du3bRnzx4tXLhQ27dvV7t27dSxY0dt2bIlByvPW4oXL66XX35ZmzZt0saNG/XQQw8pKipKO3fulCQ9++yz+vbbb/XFF19o1apVOnbsmNq1a5fDVec919sPx44d07FjxzR58mTt2LFDs2fP1tKlS9WrV6+cLvvWOeWrbHFLTp06ZSSZVatWObRv2bLFFCtWzBw/ftxIMrGxsTlT4F0io/1Qq1YtM3LkyBys6u6T0X7w9vY2H330kUO/wMBA895771ld3l2lQIEC5v333zdnz541+fPnN1988YV93u7du40ks27duhys8O5wZT9k5PPPPzdubm4mJSXF4qqcgz/jctCVQ2aBgYH2tqSkJHXp0kVvvvlmhl/EB+e7dj+cOnVK69evV1BQkOrWrasiRYqofv36+umnn3KyzDwvo/8PdevW1Weffaa///5baWlpmj9/vi5evKgGDRrkUJV5W2pqqubPn6/z58+rTp062rRpk1JSUtS4cWN7n3LlyqlEiRJat25dDlaat127HzKSkJAgPz8/ubrm+JfT35LcWXUekJaWpkGDBikyMlIVK1a0tz/77LOqW7euoqKicrC6u0dG++HAgQOSpDFjxmjy5MmqWrWqPvroIzVq1Eg7duxQmTJlcrLkPCmz/w+ff/65OnXqpIIFC8rV1VVeXl6KjY1V6dKlc7DavGf79u2qU6eOLl68KB8fH8XGxqp8+fKKj4+Xm5tbum8aL1KkiE6cOJEzxeZhme2Ha/31118aN26c+vTpkwNVOgfhI4f069dPO3bscPhreuHChVq+fDnnsy2U0X5IS0uTJPXt29f+ZYjVqlVTXFycPvzwQ8XExORIrXlZRvtBkl588UWdPXtWP/74owoVKqQFCxaoY8eOWrNmjSpVqpRD1eY9ERERio+PV0JCgr788kt1795dq1atyumy7jqZ7YerA0hiYqJatWql8uXLa8yYMTlX7O3K6fM+d6N+/fqZ4sWLmwMHDji0P/PMM8ZmsxkXFxf7JMnky5fP1K9fP2eKzcMy2w8HDhwwkszHH3/s0N6xY0fTpUsXK0u8K2S2H/bv328kmR07dji0N2rUyPTt29fKEu86jRo1Mn369DFxcXFGkjlz5ozD/BIlSpipU6fmTHF3kSv74YrExERTp04d06hRI3PhwoUcrOz2cc2HhYwx6t+/v2JjY7V8+XKFh4c7zB8+fLi2bdum+Ph4+yRJ06ZN06xZs3Kg4rzpRvuhZMmSCgkJSXfb5969exUWFmZlqXnajfZDUlKSJKW7w8jFxcV+dArZIy0tTcnJyapevbry58+vuLg4+7w9e/boyJEjmV6LAOe5sh+kf494NG3aVG5ublq4cKE8PDxyuLrblMPh567yn//8x/j7+5uVK1ea48eP26ekpKRMlxF3uzhdVvbDtGnTjJ+fn/niiy/Mvn37zMiRI42Hh4fZv39/Dlaet9xoP1y6dMmULl3aPPjgg2b9+vVm//79ZvLkycZms5lFixblcPV5x/Dhw82qVavMwYMHzbZt28zw4cONzWYzP/zwgzHGmKeeesqUKFHCLF++3GzcuNHUqVPH1KlTJ4erznuutx8SEhJMrVq1TKVKlcz+/fsd/r9cvnw5p0u/JYQPC0nKcJo1a9Z1lyF8OFdW90NMTIwpXry48fLyMnXq1DFr1qzJmYLzqKzsh71795p27dqZoKAg4+XlZSpXrpzu1lvcnieeeMKEhYUZNzc3U7hwYdOoUSN78DDGmAsXLpinn37aFChQwHh5eZm2bdua48eP52DFedP19sOKFSsy/f9y8ODBnC38FtmMMcaqoywAAABc8wEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAAS/0/ZNR6EqISRrsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for optimal threshold in range [23.1979 - 32.4159] with 100 steps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold found: 23.2910 with F1: 0.4839\n",
            "Zero-Shot CLIP Optimal Validation Metrics (Frame-Level):\n",
            "  Optimal Threshold: 23.2910\n",
            "  Precision: 0.3191, Recall: 1.0000, F1-Score: 0.4839, Accuracy: 0.3333\n",
            "  TN: 2, FP: 64, FN: 0, TP: 30\n",
            "\n",
            "--- Evaluating Fine-Tuned HL-CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating FT Model: 100%|██████████| 2/2 [00:49<00:00, 24.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned: Received 96 scores and 96 labels for threshold optimization.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for optimal threshold in range [0.4628 - 0.6052] with 100 steps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold found: 0.5049 with F1: 0.5455\n",
            "Fine-Tuned HL-CLIP Optimal Validation Metrics (Frame-Level):\n",
            "  Optimal Threshold: 0.5049\n",
            "  Precision: 0.3913, Recall: 0.9000, F1-Score: 0.5455, Accuracy: 0.5312\n",
            "  TN: 24, FP: 42, FN: 3, TP: 27\n",
            "\n",
            "Comparison Complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afQx08vndoVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "NS1D5LQhJQEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = HLCLIPModel(visual_encoder_component, num_transformer_layers_to_unfreeze=2).to(device)\n",
        "fine_tuned_model.load_state_dict(torch.load(\"best_hlclip_model.pth\", weights_only=True))\n",
        "fine_tuned_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wjB65h2JQUt",
        "outputId": "c99b7e9f-a82a-4c94-f976-ae816afab73b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HLCLIPModel(\n",
              "  (clip_visual_encoder): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Apply to specific layers of your fine_tuned_model or its components\n",
        "# Example: Pruning the first linear layer of the model's FC head (if `model.fc` is the head)\n",
        "model_to_prune = fine_tuned_model # or specific parts\n",
        "if hasattr(model_to_prune, 'fc') and isinstance(model_to_prune.fc, torch.nn.Linear):\n",
        "    prune.random_unstructured(model_to_prune.fc, name=\"weight\", amount=0.3) # Prune 30% of weights\n",
        "    prune.remove(model_to_prune.fc, 'weight') # Make pruning permanent\n",
        "    print(\"Pruned model.fc layer.\")\n",
        "\n",
        "# More generally, to prune multiple layers:\n",
        "parameters_to_prune = []\n",
        "for name, module in fine_tuned_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "        parameters_to_prune.append((module, 'weight'))\n",
        "\n",
        "if parameters_to_prune:\n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method=prune.L1Unstructured, # or RandomUnstructured\n",
        "        amount=0.2  # Prune 20% of total weights across these layers\n",
        "    )\n",
        "    # To make it permanent and remove reparameterization:\n",
        "    for module, name in parameters_to_prune:\n",
        "        prune.remove(module, name)\n",
        "    print(f\"Globally pruned {len(parameters_to_prune)} layers.\")\n",
        "\n",
        "# IMPORTANT: After pruning, you usually need to fine-tune the model for some epochs\n",
        "# to recover accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI8APJcZJQb_",
        "outputId": "710a3e6b-9f70-4314-b899-f4c2d13a9842"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model.fc layer.\n",
            "Globally pruned 39 layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Your existing training setup: model, optimizer, criterion, train_loader, etc.)\n",
        "# ... (HLCLIPModel, ElephantHighlightDataset, collate_fn, preprocess, device etc. need to be defined)\n",
        "# ... (Load CLIP, convert to float32, instantiate HLCLIPModel, optimizer, criterion as before)\n",
        "# ... (Create dummy train/val videos, train_dataset, train_loader, validation_dataset, validation_loader as shown above)\n",
        "\n",
        "model = fine_tuned_model\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "print(\"Starting training and validation loop...\")\n",
        "num_epochs = 4 # Example\n",
        "best_val_f1 = -1 # To save the best model based on F1-score, for example\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "    train_batches_processed = 0\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")):\n",
        "        # ... (your existing training batch logic) ...\n",
        "        if batch_data is None or batch_data[0] is None:\n",
        "            print(f\"Skipping empty or invalid training batch {batch_idx} from collate_fn.\")\n",
        "            continue\n",
        "        video_frames, labels = batch_data\n",
        "        if video_frames.nelement() == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to empty video_frames tensor.\")\n",
        "            continue\n",
        "\n",
        "        video_frames = video_frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(video_frames)\n",
        "\n",
        "        min_len = min(predictions.shape[1], labels.shape[1])\n",
        "        if min_len == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to min_len=0.\")\n",
        "            continue\n",
        "        predictions_aligned = predictions[:, :min_len]\n",
        "        labels_aligned = labels[:, :min_len]\n",
        "\n",
        "        if predictions_aligned.nelement() == 0 or labels_aligned.nelement() == 0:\n",
        "            print(f\"Skipping training batch {batch_idx} due to zero elements post-alignment.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            loss = criterion(predictions_aligned, labels_aligned)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "            train_batches_processed +=1\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Runtime error during training loss/backward for batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_train_loss = total_train_loss / train_batches_processed if train_batches_processed > 0 else 0\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Validation Step ---\n",
        "    if validation_loader: # Only run validation if the loader was successfully created\n",
        "        avg_val_loss, val_metrics = validate_model(model, validation_loader, criterion, device, threshold=0.5)\n",
        "        print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Epoch {epoch+1} - Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1-Score: {val_metrics['f1']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {val_metrics['tn']}, FP: {val_metrics['fp']}, FN: {val_metrics['fn']}, TP: {val_metrics['tp']}\")\n",
        "\n",
        "        # Example: Save the model if validation F1 improves\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            torch.save(model.state_dict(), \"best_prune_hlclip_model.pth\")\n",
        "            print(f\"New best validation F1: {best_val_f1:.4f}. Model saved to best_prune_hlclip_model.pth\")\n",
        "    else:\n",
        "        print(\"Skipping validation as validation_loader is not available.\")\n",
        "\n",
        "print(\"Training and validation finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnh81SbjLLSv",
        "outputId": "5e43e8f1-cb40-437d-9346-aa9b585a531d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training and validation loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4 [Training]: 100%|██████████| 10/10 [04:07<00:00, 24.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training Loss: 0.5819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:50<00:00, 25.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Validation Loss: 0.7515\n",
            "Epoch 1 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.4407, Recall: 0.8667, F1-Score: 0.5843, Accuracy: 0.6146\n",
            "  TN: 33, FP: 33, FN: 4, TP: 26\n",
            "New best validation F1: 0.5843. Model saved to best_prune_hlclip_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4 [Training]: 100%|██████████| 10/10 [03:49<00:00, 22.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Training Loss: 0.4756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:47<00:00, 23.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Validation Loss: 0.8041\n",
            "Epoch 2 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.4426, Recall: 0.9000, F1-Score: 0.5934, Accuracy: 0.6146\n",
            "  TN: 32, FP: 34, FN: 3, TP: 27\n",
            "New best validation F1: 0.5934. Model saved to best_prune_hlclip_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4 [Training]: 100%|██████████| 10/10 [03:49<00:00, 22.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Training Loss: 0.3979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:46<00:00, 23.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Validation Loss: 0.8768\n",
            "Epoch 3 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.4265, Recall: 0.9667, F1-Score: 0.5918, Accuracy: 0.5833\n",
            "  TN: 27, FP: 39, FN: 1, TP: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4 [Training]: 100%|██████████| 10/10 [03:47<00:00, 22.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Training Loss: 0.3263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 2/2 [00:48<00:00, 24.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Validation Loss: 0.8930\n",
            "Epoch 4 - Validation Metrics (Frame-Level):\n",
            "  Precision: 0.4500, Recall: 0.9000, F1-Score: 0.6000, Accuracy: 0.6250\n",
            "  TN: 33, FP: 33, FN: 3, TP: 27\n",
            "New best validation F1: 0.6000. Model saved to best_prune_hlclip_model.pth\n",
            "Training and validation finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sn7MWqmXTJIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot VS Pruned Fine-Tuning"
      ],
      "metadata": {
        "id": "3X3kZR4MTJj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_fine_tuned_model = HLCLIPModel(visual_encoder_component, num_transformer_layers_to_unfreeze=2).to(device)\n",
        "prune_fine_tuned_model.load_state_dict(torch.load(\"best_prune_hlclip_model.pth\", weights_only=True))\n",
        "prune_fine_tuned_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee8fvQPCTSDi",
        "outputId": "240af79a-f90c-481f-d116-53fa8785322d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HLCLIPModel(\n",
              "  (clip_visual_encoder): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "dummy_criterion = torch.nn.BCELoss() # For validate_model loss calculation (not used for best threshold)\n",
        "\n",
        "# --- 4. Validate Zero-Shot CLIP & Find Optimal Threshold ---\n",
        "print(\"\\n--- Evaluating Zero-Shot CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    _, zs_all_scores, zs_all_labels = validate_zero_shot_clip(\n",
        "        original_clip_model_zs, validation_loader, ZERO_SHOT_PROMPTS, device\n",
        "    )\n",
        "    if len(zs_all_scores) > 0 and len(zs_all_labels) > 0:\n",
        "        print(f\"Zero-Shot: Received {len(zs_all_scores)} scores and {len(zs_all_labels)} labels for threshold optimization.\")\n",
        "        # Plot score distribution (optional but recommended for debugging)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.hist(zs_all_scores[zs_all_labels==1], bins=50, alpha=0.5, label='ZS Positive Scores')\n",
        "        # plt.hist(zs_all_scores[zs_all_labels==0], bins=50, alpha=0.5, label='ZS Negative Scores')\n",
        "        # plt.legend()\n",
        "        # plt.title(\"Zero-Shot Score Distribution\")\n",
        "        # plt.show()\n",
        "\n",
        "        zs_optimal_metrics = find_optimal_threshold_and_metrics(zs_all_labels, zs_all_scores)\n",
        "        print(\"Zero-Shot CLIP Optimal Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Optimal Threshold: {zs_optimal_metrics['threshold']:.4f}\")\n",
        "        print(f\"  Precision: {zs_optimal_metrics['precision']:.4f}, Recall: {zs_optimal_metrics['recall']:.4f}, F1-Score: {zs_optimal_metrics['f1']:.4f}, Accuracy: {zs_optimal_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {zs_optimal_metrics['tn']}, FP: {zs_optimal_metrics['fp']}, FN: {zs_optimal_metrics['fn']}, TP: {zs_optimal_metrics['tp']}\")\n",
        "    else:\n",
        "        print(\"Zero-Shot: No scores returned from validation to find optimal threshold.\")\n",
        "else:\n",
        "    print(\"Skipping Zero-Shot validation.\")\n",
        "\n",
        "# --- 5. Validate. Pruned Fine-Tuned HL-CLIP Model & Find Optimal Threshold ---\n",
        "print(\"\\n--- Evaluating Pruned Fine-Tuned HL-CLIP Performance ---\")\n",
        "if validation_loader and len(validation_loader) > 0:\n",
        "    _, ft_all_scores, ft_all_labels = validate_model(\n",
        "        prune_fine_tuned_model, validation_loader, dummy_criterion, device\n",
        "    )\n",
        "    if len(ft_all_scores) > 0 and len(ft_all_labels) > 0:\n",
        "        print(f\"Pruned Fine-Tuned: Received {len(ft_all_scores)} scores and {len(ft_all_labels)} labels for threshold optimization.\")\n",
        "        # Plot score distribution (optional)\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.hist(ft_all_scores[ft_all_labels==1], bins=50, alpha=0.5, label='FT Positive Scores')\n",
        "        # plt.hist(ft_all_scores[ft_all_labels==0], bins=50, alpha=0.5, label='FT Negative Scores')\n",
        "        # plt.legend()\n",
        "        # plt.title(\"Fine-Tuned Score Distribution\")\n",
        "        # plt.show()\n",
        "\n",
        "        ft_optimal_metrics = find_optimal_threshold_and_metrics(ft_all_labels, ft_all_scores)\n",
        "        print(\"Pruned Fine-Tuned HL-CLIP Optimal Validation Metrics (Frame-Level):\")\n",
        "        print(f\"  Optimal Threshold: {ft_optimal_metrics['threshold']:.4f}\")\n",
        "        print(f\"  Precision: {ft_optimal_metrics['precision']:.4f}, Recall: {ft_optimal_metrics['recall']:.4f}, F1-Score: {ft_optimal_metrics['f1']:.4f}, Accuracy: {ft_optimal_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  TN: {ft_optimal_metrics['tn']}, FP: {ft_optimal_metrics['fp']}, FN: {ft_optimal_metrics['fn']}, TP: {ft_optimal_metrics['tp']}\")\n",
        "    else:\n",
        "        print(\"Pruned Fine-Tuned: No scores returned from validation to find optimal threshold.\")\n",
        "else:\n",
        "    print(\"Skipping Pruned Fine-Tuned validation.\")\n",
        "\n",
        "print(\"\\nComparison Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHk6JtVbQ3us",
        "outputId": "b39d6133-d24e-423f-b088-d3e8d6f229e7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Zero-Shot CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating Zero-Shot CLIP: 100%|██████████| 2/2 [00:48<00:00, 24.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot: Received 96 scores and 96 labels for threshold optimization.\n",
            "Searching for optimal threshold in range [23.1979 - 32.4159] with 100 steps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold found: 23.2910 with F1: 0.4839\n",
            "Zero-Shot CLIP Optimal Validation Metrics (Frame-Level):\n",
            "  Optimal Threshold: 23.2910\n",
            "  Precision: 0.3191, Recall: 1.0000, F1-Score: 0.4839, Accuracy: 0.3333\n",
            "  TN: 2, FP: 64, FN: 0, TP: 30\n",
            "\n",
            "--- Evaluating Pruned Fine-Tuned HL-CLIP Performance ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating FT Model: 100%|██████████| 2/2 [00:46<00:00, 23.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Fine-Tuned: Received 96 scores and 96 labels for threshold optimization.\n",
            "Searching for optimal threshold in range [0.3126 - 0.7610] with 100 steps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold found: 0.4951 with F1: 0.6154\n",
            "Pruned Fine-Tuned HL-CLIP Optimal Validation Metrics (Frame-Level):\n",
            "  Optimal Threshold: 0.4951\n",
            "  Precision: 0.4590, Recall: 0.9333, F1-Score: 0.6154, Accuracy: 0.6354\n",
            "  TN: 33, FP: 33, FN: 2, TP: 28\n",
            "\n",
            "Comparison Complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvp2Mc0oUfUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch JIT Compilation (TorchScript)"
      ],
      "metadata": {
        "id": "10h4TO9tXYMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_fine_tuned_model = HLCLIPModel(visual_encoder_component, num_transformer_layers_to_unfreeze=2).to(device)\n",
        "prune_fine_tuned_model.load_state_dict(torch.load(\"best_prune_hlclip_model.pth\", weights_only=True))\n",
        "prune_fine_tuned_model.eval()"
      ],
      "metadata": {
        "id": "pjJ_XncHcq1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. (Optional but Recommended) Convert to TorchScript ---\n",
        "try:\n",
        "    # Example input for tracing (adjust shape and dtype)\n",
        "    example_input_shape = (1, 32, 3, 224, 224) # (B, N_frames, C, H, W)\n",
        "    # Ensure dtype matches what your model expects after preprocess (usually float32)\n",
        "    # and device matches the model's device\n",
        "    example_input_val = torch.randn(example_input_shape, dtype=torch.float32).to(device)\n",
        "    traced_model = torch.jit.trace(prune_fine_tuned_model, example_input_val)\n",
        "    print(\"Model traced to TorchScript.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error tracing model: {e}. Proceeding with original model (might be less optimal for TensorRT).\")\n",
        "    traced_model = prune_fine_tuned_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuLBAkXDXPwK",
        "outputId": "7df056f0-a985-4082-d22d-bacc1cf0027f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model traced to TorchScript.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlights = predict_highlights(val_video_paths[0], traced_model, preprocess, device)\n",
        "print(f\"Detected highlights in {val_video_paths[0]}: {highlights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sejs5idnX9en",
        "outputId": "e1348ed6-187f-4803-ce67-c75891a90e79"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected highlights in /content/drive/MyDrive/elephant_clip/val/good/Copy of 3QbFg0aff2Q.webm: [(0.0, 1.6693376068376067), (2.003205128205128, 2.003205128205128), (2.6709401709401708, 11.351495726495726), (11.685363247863247, 13.354700854700853), (13.855502136752136, 14.022435897435896), (14.356303418803417, 14.523237179487179)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "PoOKIk6PXFde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_tensorrt # This import will only work after successful installation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# (Assuming fine_tuned_model, traced_model, device, example_input_val are set up)\n",
        "try:\n",
        "    print(\"Starting Torch-TensorRT compilation for FP16...\")\n",
        "    trt_model_fp16 = torch_tensorrt.compile(\n",
        "        traced_model, # Or fine_tuned_model\n",
        "        inputs=[torch_tensorrt.Input(\n",
        "            min_shape=example_input_shape,\n",
        "            opt_shape=example_input_shape,\n",
        "            max_shape=example_input_shape,\n",
        "            dtype=torch.float32 # Input to the TRT engine is still FP32, then converted\n",
        "        )],\n",
        "        enabled_precisions={torch.float16}, # Key change for FP16\n",
        "        workspace_size=1 << 30 # 1GB\n",
        "    )\n",
        "    print(\"Model successfully compiled with Torch-TensorRT to FP16.\")\n",
        "    # torch.jit.save(trt_model_fp16, \"trt_hlclip_fp16.ts\")\n",
        "    # Test inference\n",
        "    # output_fp16 = trt_model_fp16(example_input_val)\n",
        "    # print(\"Output from FP16 TensorRT model:\", output_fp16.shape)\n",
        "except Exception as e:\n",
        "    print(f\"Error during Torch-TensorRT FP16 compilation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "zUao-DMyXGW8",
        "outputId": "0783ed81-db42-49eb-f1c4-d8bee65ee718"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_tensorrt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b3ee49d10f30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_tensorrt\u001b[0m \u001b[0;31m# This import will only work after successful installation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (Assuming fine_tuned_model, traced_model, device, example_input_val are set up)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_tensorrt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDtVmlimaAz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}